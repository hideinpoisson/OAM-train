{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d015bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "from scipy import signal\n",
    "from scipy import misc\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373a4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_path = \"ori+TB/train/\"\n",
    "test_path = \"ori+TB/test/\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [   #transforms.CenterCrop(128),\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomRotation(10),\n",
    "#        transforms.Resize([128,128]),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "train_transform = Compose([\n",
    "    #transforms.CenterCrop(128),\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomRotation(10),\n",
    "#    transforms.Resize([128,128]),\n",
    "        transforms.ToTensor(),\n",
    "    #transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    #transforms.CenterCrop(128),\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomRotation(10),\n",
    "#    transforms.Resize([128,128]),\n",
    "        transforms.ToTensor(),\n",
    "    #transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_path,transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,drop_last=True,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=test_path,transform=transform)\n",
    "testloader =  DataLoader(testset, batch_size=BATCH_SIZE,drop_last=True,\n",
    "                                         shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca13f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 64, 64])\n",
      "torch.Size([16])\n",
      "tensor(36)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testbatch = iter(trainloader)\n",
    "testimages, testlabels = testbatch.next()\n",
    "\n",
    "print(testimages.shape)\n",
    "print(testlabels.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = 5\n",
    "img = testimages[x]\n",
    "img = torch.where(img > 0.89, img*0+1., img*0.)\n",
    "plt.imshow(img.numpy()[2])\n",
    "print(testlabels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3072c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 28 22:29:07 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.59       Driver Version: 512.59       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 53%   37C    P3    83W / 370W |   2613MiB / 10240MiB |     20%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1732    C+G   ...bat\\acrocef_1\\AcroCEF.exe    N/A      |\n",
      "|    0   N/A  N/A      1936    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A      5056    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5068    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      5320    C+G   ...lack\\app-4.25.2\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A      9876    C+G   ...er_engine\\wallpaper32.exe    N/A      |\n",
      "|    0   N/A  N/A     10328    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10632    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     10724    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     10964    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     10972    C+G   ...iginThinSetupInternal.exe    N/A      |\n",
      "|    0   N/A  N/A     13192    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     14192    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     14804    C+G   ...185.50\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     15516    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16480    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     16804    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     16844    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     17492    C+G   ...7.0.5.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     18392    C+G   ...6bftszj\\TranslucentTB.exe    N/A      |\n",
      "|    0   N/A  N/A     19172    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     19800    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78c09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          images, labels = data\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          outputs = net(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "#           _, predicted = torch.max(outputs.data, 1)  \n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('Accuracy : %f %%' % (\n",
    "      100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8432990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put net onto GPU\n",
      "FFTconv(\n",
      "  (conv1): FTconvlayer(3, 16, kernel_size=(64, 64), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc01): Linear(in_features=16384, out_features=256, bias=True)\n",
      "  (fc02): Linear(in_features=256, out_features=100, bias=True)\n",
      "  (drop_layer): Dropout(p=0.15, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.nn.modules import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "\n",
    "class _ConvNd(Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups', 'bias',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']   \n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, batch_size, stride,\n",
    "                 padding, dilation, transposed, output_padding,\n",
    "                 groups, bias, padding_mode):\n",
    "        super(_ConvNd, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                in_channels, out_channels // groups, *kernel_size))\n",
    "        else:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                out_channels, in_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        init.kaiming_normal_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(_ConvNd, self).__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "class FTconvlayer(_ConvNd):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, batch_size = 16, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        batch_size = BATCH_SIZE\n",
    "        kernel_size = _pair(kernel_size) \n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(FTconvlayer, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, batch_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias, padding_mode)\n",
    "\n",
    "    def quantization_n(self, input, n = 1, max = 1):\n",
    "      intv = max/(2**n-1)\n",
    "      qunt = torch.ceil(torch.mul(input,(1/intv)))  #***use ceil instead of floor for small value of n to make sure that not all weights are modified to zero in the beginning, achieves good accuracy for small n\n",
    "      #the above line divide the whole tensor by the smallest interval (1/2**n-1), which is same as multiply with 2**n-1, then take the floor and finally multiply the whole tensor with the smallest interval\n",
    "      out = torch.mul(qunt,intv)\n",
    "      out = torch.clamp(out, min=0, max=max) #make sure the quantized version lies in the interval 0-1, if it's bigger than one just clamp it at one\n",
    "      return(out)  \n",
    "\n",
    "    def input_quant(self, input, level = 5):\n",
    "        max = torch.max(input)\n",
    "        intv = max/level\n",
    "        qunt = torch.floor(torch.mul(input,(1/intv)))\n",
    "        out = torch.mul(qunt, intv)\n",
    "        out = torch.clamp(out, min=0, max=max)\n",
    "        return(out)\n",
    "    \n",
    "    def weightclamp(self, input):\n",
    "      return input.clamp_(0)\n",
    "\n",
    "    def make_complex(self, x):  #converts a real tensor into complex form by adding one extra dimension to it\n",
    "        #x_i = torch.zeros(x.shape)\n",
    "        x_i = torch.cuda.FloatTensor(x.shape).fill_(0)\n",
    "        y = torch.stack((x,x_i),-1)\n",
    "        return torch.view_as_complex(y)\n",
    "\n",
    "    def neg_complex_exp(self, x):    #since pytorch does not support complex exponential, implemented using euler formula exp(-jx)=cos(-x)+jsin(-x)\n",
    "        x_cos = torch.cos(-x)\n",
    "        x_sin = torch.sin(-x)\n",
    "        x_euler = torch.stack((x_cos, x_sin), -1)\n",
    "        return torch.view_as_complex(x_euler)\n",
    "\n",
    "    def complex_mul(self, x,y):  #this implementation should support broadcasting \n",
    "        #result_r = x[...,0] * y[...,0]-x[...,1] * y[...,1]#real\n",
    "        #result_i = x[...,0] * y[...,1]+x[...,1] * y[...,0]#complex\n",
    "        #result = torch.stack((result_r,result_i),-1)#stack them together to get the result\n",
    "        result = x*y\n",
    "        return result\n",
    "\n",
    "    def conj_transpose(self, x):  #should support broadcasting\n",
    "        x = torch.view_as_real(x)\n",
    "        size = len(x.size())\n",
    "        x_r = x[...,0]\n",
    "        x_i = x[...,1]\n",
    "        x_i_c =-x_i\n",
    "        x_conj = torch.stack((x_r,x_i_c),-1)\n",
    "        x_conj_t = torch.transpose(x_conj, size-3, size-2)#size-1 is the dimension for complex representation\n",
    "        return torch.view_as_complex(x_conj_t)\n",
    "\n",
    "    def roll_n(self, X, axis, n):\n",
    "        f_idx = tuple(slice(None, None, None) if i != axis else slice(0, n, None) for i in range(X.dim()))\n",
    "        b_idx = tuple(slice(None, None, None) if i != axis else slice(n, None, None) for i in range(X.dim()))\n",
    "        front = X[f_idx]\n",
    "        back = X[b_idx]\n",
    "        return torch.cat([back, front], axis)   \n",
    "\n",
    "    def batch_fftshift2d(self, x):\n",
    "        real, imag = torch.unbind(x, -1)\n",
    "        for dim in range(len(real.size())-2, len(real.size())):\n",
    "            n_shift = real.size(dim)//2\n",
    "            if real.size(dim) % 2 != 0:\n",
    "                n_shift += 1  # for odd-sized images\n",
    "            real = self.roll_n(real, axis=dim, n=n_shift)\n",
    "            imag = self.roll_n(imag, axis=dim, n=n_shift)\n",
    "        return torch.stack((real, imag), -1)  # last dim=2 (real&imag)\n",
    "\n",
    "\n",
    "    def batch_ifftshift2d(self, x):\n",
    "        real, imag = torch.unbind(x, -1)\n",
    "        for dim in range(len(real.size()) - 1, len(real.size())-3, -1):\n",
    "            real = self.roll_n(real, axis=dim, n=real.size(dim)//2)\n",
    "            imag = self.roll_n(imag, axis=dim, n=imag.size(dim)//2)\n",
    "        return torch.stack((real, imag), -1)  # last dim=2 (real&imag)\n",
    "\n",
    "    def propTF(self, u1,L,lambdaa,z):\n",
    "        batch,M,N = u1.shape\n",
    "        dx = L/M\n",
    "        fx = torch.arange(-1/(2*dx),1/(2*dx), 1/L).cuda()\n",
    "        FX,FY = torch.meshgrid(fx,fx)\n",
    "        #H = torch.exp(-1j*math.pi*lambdaa*z*(FX**2+FY**2))\n",
    "        H = self.neg_complex_exp(math.pi*lambdaa*z*(FX**2+FY**2))\n",
    "        #H = self.batch_fftshift2d(H)\n",
    "        H = torch.fft.fftshift(H)\n",
    "        U1 = torch.fft.fft2(torch.fft.fftshift(u1)) #so the U1's dimension is 4, H is 3 so complex_mul needs to support broadcasting\n",
    "        U2 = self.complex_mul(H,U1)  ####GENERATES ERROR\n",
    "        u2 = torch.fft.ifftshift(torch.fft.ifft2(U2)) #this step is redundent, can be removed in actual implementation\n",
    "        return u2\n",
    "\n",
    "\n",
    "    def seidel_5(self, u0, v0, X, Y, wd, w040, w131, w222, w220, w311):\n",
    "        beta = math.atan2(u0,v0)\n",
    "        u0r=math.sqrt(u0**2+v0**2)\n",
    "        Xr=X*math.cos(beta)+Y*math.sin(beta)\n",
    "        Yr=-X*math.sin(beta)+Y*math.cos(beta)\n",
    "        rho2=Xr**2+Yr**2\n",
    "        w=wd*rho2+w040*rho2**2+w131*u0r*rho2*Xr+ w222*u0r**2*Xr**2+w220*u0r**2*rho2+w311*math.pow(u0r,3)*Xr\n",
    "        return w\n",
    "\n",
    "    def circ(self, r):\n",
    "        out = torch.abs(r)<=1\n",
    "        return out\n",
    "\n",
    "    ''' for block mean pytorch does not support reshape using 'F' ordering, so use normal reshape and then permute'''\n",
    "    def blockmean_batch(self, X, V, W):\n",
    "        S=X.shape\n",
    "        B1 = S[0]\n",
    "        B2 = S[1]\n",
    "        M = int(S[2] - S[2]%V)\n",
    "        N = int(S[3] - S[3]%W)\n",
    "        if(M*N == 0):\n",
    "            Y = X\n",
    "            return Y\n",
    "        MV = int(M/V)  \n",
    "        NW = int(N/W)\n",
    "        #XM = np.reshape(X[0:M, 0:N, :],(V, MV, W, NW, -1))\n",
    "        #XM =  X[0:M, 0:N].reshape(V, MV, W, NW, order=\"F\")\n",
    "        XM = X[:,:,0:M, 0:N].permute(0,1,3,2).reshape([B1,B2,NW, W, MV, V]).permute(0,1,5,4,3,2)\n",
    "        #three version of Y in matlab function depends on differen type of inputs, here stick to the double case\n",
    "        Y = torch.sum(torch.sum(XM,2),3) * (1/(V*W))\n",
    "        return Y\n",
    "\n",
    "    def extract_result(self,input,img_size):\n",
    "        size = input.shape[-1]\n",
    "        start = int((size-4*img_size)/2)\n",
    "        end = start + 4*img_size\n",
    "        output = input[:,:,start:end,start:end]\n",
    "        return output\n",
    "    \n",
    "    def input_pad(self,input,padsize):\n",
    "      input_size = input.shape[2]\n",
    "      pad_size_x = int((padsize-input_size)/2)\n",
    "      pad_size_y = int((padsize-input_size)/2)\n",
    "      p2d = (pad_size_x, pad_size_y, pad_size_x, pad_size_y)\n",
    "      input_pad = F.pad(input, p2d, \"constant\", 0)\n",
    "      return input_pad \n",
    "\n",
    "    def input_adjust(self, input):\n",
    "      input = torch.mul(input,5)\n",
    "      input = torch.floor(input)\n",
    "      output = torch.clamp(input,min=0,max=1)\n",
    "      return(output)\n",
    "\n",
    "    def evenkernel(self, input):\n",
    "      uptri = torch.triu(input,diagonal = 1)\n",
    "      downtri = torch.flip(torch.triu(input,diagonal = 1),[1,2])\n",
    "      result = uptri+downtri\n",
    "      return result\n",
    "\n",
    "    def extract_result(self,input,img_size):\n",
    "      size = input.shape[-1]\n",
    "      start = int((size-img_size)/2)\n",
    "      end = start + img_size\n",
    "      output = input[:,:,start:end,start:end]\n",
    "      return output\n",
    "    \n",
    "    def norm(self,input):\n",
    "      size = input.shape\n",
    "      output = torch.cuda.FloatTensor(size).fill_(0)\n",
    "      for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "          orig = input[i,j,:,:]\n",
    "          maxi = torch.max(orig)\n",
    "          mini = torch.min(orig)\n",
    "          output[i,j,:,:] = (orig-mini)/(maxi-mini)\n",
    "      return output\n",
    "\n",
    "    def kernel_even(self,input):\n",
    "      input_transpose = torch.transpose(input, 1,2)\n",
    "      input = input + input_transpose\n",
    "      return input\n",
    "\n",
    "    def kernel_hpf_even(self, input, amount):\n",
    "      # make the center part of the weight to be zero and make the filter symmetrical\n",
    "      # This function should be placed before quantization\n",
    "      # amount is the size of the area that are set to 0\n",
    "      mid = int(input.shape[2]/2)\n",
    "      input[:,mid-amount:mid+amount,mid-amount:mid+amount] = 0 #set the center part to be zero\n",
    "\n",
    "      input_transpose = torch.transpose(input, 1,2)\n",
    "      input = (input + input_transpose)/2\n",
    "      #even = input\n",
    "      return input\n",
    "\n",
    "    def accurate_model_forward(self, input, weight):\n",
    "        err = 1e-8 #define a very small error term to aviod nan loss due to abs\n",
    "        #print(input[0, ...])\n",
    "        #code to quantize the input to certain intervals to simulate dmd\n",
    "        #with torch.no_grad():\n",
    "          #input = self.input_quant(input)\n",
    "        res = input.shape[2]\n",
    "        xx = res\n",
    "        yy = xx \n",
    "        w = res    \n",
    "        #output_arr = torch.empty(32,16,w,w) #output dimension of the ftconv layer, hard-coded for easier implementation, 28 for MNIST and 32 for CIFAR\n",
    "        n_filter_actual = int(self.out_channels/2)\n",
    "        output_full = torch.cuda.FloatTensor(BATCH_SIZE,self.out_channels,w,w).fill_(0)\n",
    "        output_sub = torch.cuda.FloatTensor(BATCH_SIZE, int(self.out_channels/2), w, w).fill_(0)\n",
    "        # Define unit matrix DMD \n",
    "        '''unitmatrix = torch.cuda.FloatTensor(17, 17).fill_(0)\n",
    "        unitmatrix[1:16,1:16]=1\n",
    "        unitmatrix[7:10,7:10]=0\n",
    "        idledmd = unitmatrix.repeat(xx,yy)\n",
    "        '''\n",
    "        idledmd = torch.cuda.FloatTensor(res, res).fill_(1)\n",
    "        M,N = idledmd.shape\n",
    "        L1=1.90e-2*xx/res\n",
    "        L2=1.09e-2*yy/res\n",
    "        du=L1/M\n",
    "        dv=L2/N\n",
    "        lambdaa = 0.633e-6\n",
    "        k=2*math.pi/lambdaa\n",
    "        \n",
    "        '''Lens Diffraction (Aperture) and Aberration'''\n",
    "        fu = torch.arange(-1/(2*du),1/(2*du),1/L1)\n",
    "        #fv = torch.arange(-1/(2*dv),1/(2*dv),1/L2)\n",
    "        fv = torch.arange(-1/(2*dv),1/(2*dv),1/L2)\n",
    "        Dxp = 5e-2\n",
    "        wxp = Dxp/2\n",
    "        zxp = 200e-3\n",
    "        lz = lambdaa*zxp\n",
    "        u0 = 0\n",
    "        v0 = 0\n",
    "        f0 = wxp/(lambdaa*zxp)\n",
    "        '''Lens parameter for aberration (Seidel coefficients), wavefront alteration from spherical waves'''\n",
    "        wd=0*lambdaa\n",
    "        w040=4.963*lambdaa\n",
    "        w131=2.637*lambdaa\n",
    "        w222=9.025*lambdaa\n",
    "        w220=7.536*2*lambdaa\n",
    "        w311=0.157*12*lambdaa\n",
    "        \n",
    "        Fu,Fv = torch.meshgrid(fu,fv)\n",
    "        Fu = torch.transpose(Fu,0,1)\n",
    "        Fv = torch.transpose(Fv,0,1)\n",
    "        W = self.seidel_5(u0,v0,-lz*Fu/wxp,-lz*Fv/wxp,wd,w040,w131,w222,w220,w311).cuda() #same as the matlab calculation\n",
    "        #H = circ(torch.sqrt(Fu**2 + Fv**2)/f0)*torch.exp(-1j*k*W)#same as matlab calculation\n",
    "        H = self.complex_mul(self.make_complex(self.circ(torch.sqrt(Fu**2 + Fv**2)/f0).float().cuda()),self.neg_complex_exp(k*W))\n",
    "#-----------------------from here is the actual training, before the loop is basically constants/parameters genreation, which does not needs to be backproped     \n",
    "        for c_in in range(input.shape[1]): # iters for number of input channels\n",
    "            signal = input[:,c_in,:,:] #the dimension of signal is 3, with one batch dimension\n",
    "            weight_raw = weight[:,c_in,:,:] #the dimension of weights are now 3\n",
    "            #apply high pass filter and make kernel even\n",
    "            #weight_raw.data = self.kernel_even(weight_raw.data)\n",
    "            weight_raw.data = self.kernel_hpf_even(weight_raw.data,3)\n",
    "            weight_raw.data = self.quantization_n(weight_raw.data, 1, 1)\n",
    "            #print(weight_raw[0,...])\n",
    "            #weight_raw.data = torch.clamp(weight_raw.data, min=0, max=1)\n",
    "            '''interleave dimension needs to be changed since now the first dimension is batch dimension'''\n",
    "            #sw = torch.repeat_interleave(signal,4, dim=1)\n",
    "            #sw = torch.repeat_interleave(sw,4, dim=2)\n",
    "            dmd_1 = self.make_complex(self.input_pad(signal,res)) #dimension of dmd1 is now 4, first dimension is now batch dimension, so propTF needs to be changed accordingly\n",
    "            '''interleave dimension also needs to be changed here'''\n",
    "            #kk = torch.repeat_interleave(weight_raw,20, dim=1)#due to alignment reason, adjust the minimum unit of kernels to 50*50 pixles, so effective kernek size is 84*84\n",
    "            #kk = torch.repeat_interleave(kk,20, dim=2)\n",
    "            #dmd_kk = self.input_pad(kk,res)\n",
    "            #dmd_kk = self.evenkernel(weight_raw)\n",
    "            #dmd_kk = weight_raw\n",
    "            \n",
    "            #now need to implement propTF function\n",
    "            u2 = self.propTF(dmd_1,1.9e-2, lambdaa, 1.9e-2)\n",
    "            #u2 = dmd_1\n",
    "            \n",
    "            '''Fourier Transform after first lens'''\n",
    "            Gg = torch.fft.fftshift(torch.fft.fft2(u2))\n",
    "            Gi = self.complex_mul(Gg,self.conj_transpose(H))\n",
    "            Gi = Gi.unsqueeze(1)\n",
    "            \n",
    "            '''dot product in the fourier plane'''\n",
    "            Gii = self.complex_mul(Gi,self.make_complex(weight_raw))\n",
    "            '''Then get the result in real space'''\n",
    "            Grs = torch.fft.ifft2(torch.fft.ifftshift(Gii))\n",
    "            Grs = torch.view_as_real(Grs)\n",
    "            #Ii = torch.abs(Grs)**2\n",
    "            Ii = torch.sqrt(Grs[...,0]**2+Grs[...,1]**2+err)\n",
    "            #Ii = Grs[...,0]**2+Grs[...,1]**2 #take the squre to represent light intensity\n",
    "            op_abs = Ii#op_abs = self.extract_result(Ii,32)\n",
    "            output_full += op_abs\n",
    "        #output_sub = output_full[:, 0:n_filter_actual, :, :] - output_full[:, n_filter_actual:, :, :]\n",
    "        return output_full\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.accurate_model_forward(input, self.weight)\n",
    "        \n",
    "        #shape of input is [32,1,28,28], shape of kernel(weight) is [16,1,5,5]\n",
    "\n",
    "\n",
    "class FFTconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFTconv, self).__init__()\n",
    "        self.conv1 = FTconvlayer(3, 16, 64) #outdimension should be [32,16,28,28] (1/3,:,5/28/32) depends on whether training in fourier domain or no\n",
    "        #self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.fc01 = nn.Linear(16 * 32 * 32, 256)   #self.fc01 = nn.Linear(16 * 32 * 32, 256)\n",
    "        self.fc02 = nn.Linear(256, 100) #class number\n",
    "        self.drop_layer = nn.Dropout(p=0.15)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = torch.where(x > 0.9, x*0+1., x*0.)\n",
    "        x = self.bn1(self.pool1(self.conv1(x)))\n",
    "        x = x.view(16, 16 * 32 * 32)  #NEEDS UPDATE x = x.view(64, 16 * 32 * 32)\n",
    "        #x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc01(x))\n",
    "        #x = self.drop_layer(x)\n",
    "        x = self.fc02(x)\n",
    "        return x\n",
    "\n",
    "net = FFTconv()    \n",
    "\n",
    "\n",
    "if device:\n",
    "    net.to(device)\n",
    "    #torch.backends.cudnn.benchmark = True\n",
    "    print(\"put net onto GPU\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a263598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.000390625, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.000390625) #default learning rate for adam is 0.001\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, init_lr, freq):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every n epochs\"\"\"\n",
    "    lr = init_lr * (0.3 ** (epoch // freq))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f085b8e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\GAN\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 2.856\n",
      "[1,   400] loss: 1.342\n",
      "[1,   600] loss: 0.854\n",
      "[1,   800] loss: 0.511\n",
      "[1,  1000] loss: 0.336\n",
      "[1,  1200] loss: 0.251\n",
      "[1,  1400] loss: 0.215\n",
      "[1,  1600] loss: 0.179\n",
      "[1,  1800] loss: 0.130\n",
      "[1,  2000] loss: 0.123\n",
      "[1,  2200] loss: 0.144\n",
      "[1,  2400] loss: 0.142\n",
      "[1,  2600] loss: 0.148\n",
      "[1,  2800] loss: 0.124\n",
      "[1,  3000] loss: 0.105\n",
      "[1,  3200] loss: 0.097\n",
      "[1,  3400] loss: 0.110\n",
      "[1,  3600] loss: 0.078\n",
      "[1,  3800] loss: 0.093\n",
      "[1,  4000] loss: 0.094\n",
      "[1,  4200] loss: 0.117\n",
      "[1,  4400] loss: 0.064\n",
      "[1,  4600] loss: 0.083\n",
      "[1,  4800] loss: 0.045\n",
      "[1,  5000] loss: 0.087\n",
      "[1,  5200] loss: 0.078\n",
      "[1,  5400] loss: 0.057\n",
      "[1,  5600] loss: 0.055\n",
      "Accuracy : 94.423333 %\n",
      "Training epoch  1\n",
      "[2,   200] loss: 0.062\n",
      "[2,   400] loss: 0.054\n",
      "[2,   600] loss: 0.049\n",
      "[2,   800] loss: 0.086\n",
      "[2,  1000] loss: 0.047\n",
      "[2,  1200] loss: 0.074\n",
      "[2,  1400] loss: 0.048\n",
      "[2,  1600] loss: 0.039\n",
      "[2,  1800] loss: 0.068\n",
      "[2,  2000] loss: 0.033\n",
      "[2,  2200] loss: 0.073\n",
      "[2,  2400] loss: 0.038\n",
      "[2,  2600] loss: 0.034\n",
      "[2,  2800] loss: 0.062\n",
      "[2,  3000] loss: 0.047\n",
      "[2,  3200] loss: 0.026\n",
      "[2,  3400] loss: 0.026\n",
      "[2,  3600] loss: 0.051\n",
      "[2,  3800] loss: 0.099\n",
      "[2,  4000] loss: 0.053\n",
      "[2,  4200] loss: 0.014\n",
      "[2,  4400] loss: 0.035\n",
      "[2,  4600] loss: 0.068\n",
      "[2,  4800] loss: 0.064\n",
      "[2,  5000] loss: 0.031\n",
      "[2,  5200] loss: 0.040\n",
      "[2,  5400] loss: 0.024\n",
      "[2,  5600] loss: 0.035\n",
      "Accuracy : 96.196667 %\n",
      "Training epoch  2\n",
      "[3,   200] loss: 0.042\n",
      "[3,   400] loss: 0.030\n",
      "[3,   600] loss: 0.052\n",
      "[3,   800] loss: 0.051\n",
      "[3,  1000] loss: 0.034\n",
      "[3,  1200] loss: 0.043\n",
      "[3,  1400] loss: 0.022\n",
      "[3,  1600] loss: 0.022\n",
      "[3,  1800] loss: 0.024\n",
      "[3,  2000] loss: 0.035\n",
      "[3,  2200] loss: 0.065\n",
      "[3,  2400] loss: 0.035\n",
      "[3,  2600] loss: 0.021\n",
      "[3,  2800] loss: 0.026\n",
      "[3,  3000] loss: 0.036\n",
      "[3,  3200] loss: 0.045\n",
      "[3,  3400] loss: 0.048\n",
      "[3,  3600] loss: 0.020\n",
      "[3,  3800] loss: 0.027\n",
      "[3,  4000] loss: 0.025\n",
      "[3,  4200] loss: 0.021\n",
      "[3,  4400] loss: 0.027\n",
      "[3,  4600] loss: 0.026\n",
      "[3,  4800] loss: 0.018\n",
      "[3,  5000] loss: 0.008\n",
      "[3,  5200] loss: 0.066\n",
      "[3,  5400] loss: 0.053\n",
      "[3,  5600] loss: 0.024\n",
      "Accuracy : 96.590000 %\n",
      "Training epoch  3\n",
      "[4,   200] loss: 0.016\n",
      "[4,   400] loss: 0.025\n",
      "[4,   600] loss: 0.032\n",
      "[4,   800] loss: 0.028\n",
      "[4,  1000] loss: 0.023\n",
      "[4,  1200] loss: 0.027\n",
      "[4,  1400] loss: 0.025\n",
      "[4,  1600] loss: 0.018\n",
      "[4,  1800] loss: 0.026\n",
      "[4,  2000] loss: 0.026\n",
      "[4,  2200] loss: 0.011\n",
      "[4,  2400] loss: 0.022\n",
      "[4,  2600] loss: 0.040\n",
      "[4,  2800] loss: 0.033\n",
      "[4,  3000] loss: 0.025\n",
      "[4,  3200] loss: 0.015\n",
      "[4,  3400] loss: 0.014\n",
      "[4,  3600] loss: 0.022\n",
      "[4,  3800] loss: 0.032\n",
      "[4,  4000] loss: 0.019\n",
      "[4,  4200] loss: 0.028\n",
      "[4,  4400] loss: 0.027\n",
      "[4,  4600] loss: 0.024\n",
      "[4,  4800] loss: 0.021\n",
      "[4,  5000] loss: 0.020\n",
      "[4,  5200] loss: 0.025\n",
      "[4,  5400] loss: 0.014\n",
      "[4,  5600] loss: 0.013\n",
      "Accuracy : 96.800000 %\n",
      "Training epoch  4\n",
      "[5,   200] loss: 0.034\n",
      "[5,   400] loss: 0.016\n",
      "[5,   600] loss: 0.025\n",
      "[5,   800] loss: 0.016\n",
      "[5,  1000] loss: 0.013\n",
      "[5,  1200] loss: 0.025\n",
      "[5,  1400] loss: 0.017\n",
      "[5,  1600] loss: 0.028\n",
      "[5,  1800] loss: 0.027\n",
      "[5,  2000] loss: 0.012\n",
      "[5,  2200] loss: 0.008\n",
      "[5,  2400] loss: 0.015\n",
      "[5,  2600] loss: 0.018\n",
      "[5,  2800] loss: 0.013\n",
      "[5,  3000] loss: 0.006\n",
      "[5,  3200] loss: 0.017\n",
      "[5,  3400] loss: 0.021\n",
      "[5,  3600] loss: 0.014\n",
      "[5,  3800] loss: 0.031\n",
      "[5,  4000] loss: 0.020\n",
      "[5,  4200] loss: 0.033\n",
      "[5,  4400] loss: 0.014\n",
      "[5,  4600] loss: 0.012\n",
      "[5,  4800] loss: 0.006\n",
      "[5,  5000] loss: 0.023\n",
      "[5,  5200] loss: 0.018\n",
      "[5,  5400] loss: 0.011\n",
      "[5,  5600] loss: 0.016\n",
      "Accuracy : 97.386667 %\n",
      "Training epoch  5\n",
      "[6,   200] loss: 0.009\n",
      "[6,   400] loss: 0.017\n",
      "[6,   600] loss: 0.035\n",
      "[6,   800] loss: 0.023\n",
      "[6,  1000] loss: 0.009\n",
      "[6,  1200] loss: 0.014\n",
      "[6,  1400] loss: 0.025\n",
      "[6,  1600] loss: 0.010\n",
      "[6,  1800] loss: 0.016\n",
      "[6,  2000] loss: 0.022\n",
      "[6,  2200] loss: 0.011\n",
      "[6,  2400] loss: 0.019\n",
      "[6,  2600] loss: 0.025\n",
      "[6,  2800] loss: 0.007\n",
      "[6,  3000] loss: 0.009\n",
      "[6,  3200] loss: 0.025\n",
      "[6,  3400] loss: 0.017\n",
      "[6,  3600] loss: 0.008\n",
      "[6,  3800] loss: 0.009\n",
      "[6,  4000] loss: 0.019\n",
      "[6,  4200] loss: 0.019\n",
      "[6,  4400] loss: 0.033\n",
      "[6,  4600] loss: 0.016\n",
      "[6,  4800] loss: 0.015\n",
      "[6,  5000] loss: 0.010\n",
      "[6,  5200] loss: 0.012\n",
      "[6,  5400] loss: 0.002\n",
      "[6,  5600] loss: 0.006\n",
      "Accuracy : 97.940000 %\n",
      "Training epoch  6\n",
      "[7,   200] loss: 0.005\n",
      "[7,   400] loss: 0.010\n",
      "[7,   600] loss: 0.005\n",
      "[7,   800] loss: 0.030\n",
      "[7,  1000] loss: 0.019\n",
      "[7,  1200] loss: 0.022\n",
      "[7,  1400] loss: 0.010\n",
      "[7,  1600] loss: 0.013\n",
      "[7,  1800] loss: 0.009\n",
      "[7,  2000] loss: 0.004\n",
      "[7,  2200] loss: 0.003\n",
      "[7,  2400] loss: 0.007\n",
      "[7,  2600] loss: 0.011\n",
      "[7,  2800] loss: 0.015\n",
      "[7,  3000] loss: 0.007\n",
      "[7,  3200] loss: 0.014\n",
      "[7,  3400] loss: 0.013\n",
      "[7,  3600] loss: 0.013\n",
      "[7,  3800] loss: 0.007\n",
      "[7,  4000] loss: 0.008\n",
      "[7,  4200] loss: 0.008\n",
      "[7,  4400] loss: 0.016\n",
      "[7,  4600] loss: 0.021\n",
      "[7,  4800] loss: 0.009\n",
      "[7,  5000] loss: 0.009\n",
      "[7,  5200] loss: 0.021\n",
      "[7,  5400] loss: 0.015\n",
      "[7,  5600] loss: 0.028\n",
      "Accuracy : 97.423333 %\n",
      "Training epoch  7\n",
      "[8,   200] loss: 0.011\n",
      "[8,   400] loss: 0.002\n",
      "[8,   600] loss: 0.003\n",
      "[8,   800] loss: 0.014\n",
      "[8,  1000] loss: 0.007\n",
      "[8,  1200] loss: 0.006\n",
      "[8,  1400] loss: 0.015\n",
      "[8,  1600] loss: 0.011\n",
      "[8,  1800] loss: 0.007\n",
      "[8,  2000] loss: 0.007\n",
      "[8,  2200] loss: 0.010\n",
      "[8,  2400] loss: 0.007\n",
      "[8,  2600] loss: 0.021\n",
      "[8,  2800] loss: 0.038\n",
      "[8,  3000] loss: 0.012\n",
      "[8,  3200] loss: 0.004\n",
      "[8,  3400] loss: 0.005\n",
      "[8,  3600] loss: 0.014\n",
      "[8,  3800] loss: 0.005\n",
      "[8,  4000] loss: 0.012\n",
      "[8,  4200] loss: 0.009\n",
      "[8,  4400] loss: 0.012\n",
      "[8,  4600] loss: 0.005\n",
      "[8,  4800] loss: 0.006\n",
      "[8,  5000] loss: 0.008\n",
      "[8,  5200] loss: 0.013\n",
      "[8,  5400] loss: 0.011\n",
      "[8,  5600] loss: 0.014\n",
      "Accuracy : 97.170000 %\n",
      "Training epoch  8\n",
      "[9,   200] loss: 0.019\n",
      "[9,   400] loss: 0.013\n",
      "[9,   600] loss: 0.007\n",
      "[9,   800] loss: 0.016\n",
      "[9,  1000] loss: 0.016\n",
      "[9,  1200] loss: 0.013\n",
      "[9,  1400] loss: 0.009\n",
      "[9,  1600] loss: 0.005\n",
      "[9,  1800] loss: 0.012\n",
      "[9,  2000] loss: 0.008\n",
      "[9,  2200] loss: 0.015\n",
      "[9,  2400] loss: 0.005\n",
      "[9,  2600] loss: 0.005\n",
      "[9,  2800] loss: 0.002\n",
      "[9,  3000] loss: 0.009\n",
      "[9,  3200] loss: 0.004\n",
      "[9,  3400] loss: 0.010\n",
      "[9,  3600] loss: 0.014\n",
      "[9,  3800] loss: 0.012\n",
      "[9,  4000] loss: 0.007\n",
      "[9,  4200] loss: 0.014\n",
      "[9,  4400] loss: 0.007\n",
      "[9,  4600] loss: 0.005\n",
      "[9,  4800] loss: 0.006\n",
      "[9,  5000] loss: 0.006\n",
      "[9,  5200] loss: 0.010\n",
      "[9,  5400] loss: 0.015\n",
      "[9,  5600] loss: 0.019\n",
      "Accuracy : 97.026667 %\n",
      "Training epoch  9\n",
      "[10,   200] loss: 0.012\n",
      "[10,   400] loss: 0.022\n",
      "[10,   600] loss: 0.020\n",
      "[10,   800] loss: 0.004\n",
      "[10,  1000] loss: 0.020\n",
      "[10,  1200] loss: 0.007\n",
      "[10,  1400] loss: 0.012\n",
      "[10,  1600] loss: 0.009\n",
      "[10,  1800] loss: 0.007\n",
      "[10,  2000] loss: 0.004\n",
      "[10,  2200] loss: 0.005\n",
      "[10,  2400] loss: 0.011\n",
      "[10,  2600] loss: 0.014\n",
      "[10,  2800] loss: 0.011\n",
      "[10,  3000] loss: 0.011\n",
      "[10,  3200] loss: 0.003\n",
      "[10,  3400] loss: 0.011\n",
      "[10,  3600] loss: 0.011\n",
      "[10,  3800] loss: 0.007\n",
      "[10,  4000] loss: 0.011\n",
      "[10,  4200] loss: 0.017\n",
      "[10,  4400] loss: 0.003\n",
      "[10,  4600] loss: 0.003\n",
      "[10,  4800] loss: 0.007\n",
      "[10,  5000] loss: 0.007\n",
      "[10,  5200] loss: 0.004\n",
      "[10,  5400] loss: 0.008\n",
      "[10,  5600] loss: 0.014\n",
      "Accuracy : 97.680000 %\n",
      "Training epoch  10\n",
      "[11,   200] loss: 0.010\n",
      "[11,   400] loss: 0.003\n",
      "[11,   600] loss: 0.002\n",
      "[11,   800] loss: 0.004\n",
      "[11,  1000] loss: 0.005\n",
      "[11,  1200] loss: 0.002\n",
      "[11,  1400] loss: 0.000\n",
      "[11,  1600] loss: 0.001\n",
      "[11,  1800] loss: 0.004\n",
      "[11,  2000] loss: 0.002\n",
      "[11,  2200] loss: 0.001\n",
      "[11,  2400] loss: 0.000\n",
      "[11,  2600] loss: 0.002\n",
      "[11,  2800] loss: 0.004\n",
      "[11,  3000] loss: 0.004\n",
      "[11,  3200] loss: 0.001\n",
      "[11,  3400] loss: 0.001\n",
      "[11,  3600] loss: 0.002\n",
      "[11,  3800] loss: 0.003\n",
      "[11,  4000] loss: 0.001\n",
      "[11,  4200] loss: 0.000\n",
      "[11,  4400] loss: 0.001\n",
      "[11,  4600] loss: 0.000\n",
      "[11,  4800] loss: 0.002\n",
      "[11,  5000] loss: 0.003\n",
      "[11,  5200] loss: 0.003\n",
      "[11,  5400] loss: 0.001\n",
      "[11,  5600] loss: 0.001\n",
      "Accuracy : 98.196667 %\n",
      "Training epoch  11\n",
      "[12,   200] loss: 0.000\n",
      "[12,   400] loss: 0.000\n",
      "[12,   600] loss: 0.001\n",
      "[12,   800] loss: 0.000\n",
      "[12,  1000] loss: 0.001\n",
      "[12,  1200] loss: 0.002\n",
      "[12,  1400] loss: 0.000\n",
      "[12,  1600] loss: 0.000\n",
      "[12,  1800] loss: 0.001\n",
      "[12,  2000] loss: 0.001\n",
      "[12,  2200] loss: 0.000\n",
      "[12,  2400] loss: 0.002\n",
      "[12,  2600] loss: 0.002\n",
      "[12,  2800] loss: 0.003\n",
      "[12,  3000] loss: 0.000\n",
      "[12,  3200] loss: 0.002\n",
      "[12,  3400] loss: 0.003\n",
      "[12,  3600] loss: 0.001\n",
      "[12,  3800] loss: 0.001\n",
      "[12,  4000] loss: 0.004\n",
      "[12,  4200] loss: 0.001\n",
      "[12,  4400] loss: 0.002\n",
      "[12,  4600] loss: 0.001\n",
      "[12,  4800] loss: 0.001\n",
      "[12,  5000] loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12,  5200] loss: 0.001\n",
      "[12,  5400] loss: 0.001\n",
      "[12,  5600] loss: 0.003\n",
      "Accuracy : 97.863333 %\n",
      "Training epoch  12\n",
      "[13,   200] loss: 0.004\n",
      "[13,   400] loss: 0.001\n",
      "[13,   600] loss: 0.001\n",
      "[13,   800] loss: 0.002\n",
      "[13,  1000] loss: 0.001\n",
      "[13,  1200] loss: 0.001\n",
      "[13,  1400] loss: 0.001\n",
      "[13,  1600] loss: 0.002\n",
      "[13,  1800] loss: 0.000\n",
      "[13,  2000] loss: 0.000\n",
      "[13,  2200] loss: 0.000\n",
      "[13,  2400] loss: 0.002\n",
      "[13,  2600] loss: 0.002\n",
      "[13,  2800] loss: 0.004\n",
      "[13,  3000] loss: 0.000\n",
      "[13,  3200] loss: 0.000\n",
      "[13,  3400] loss: 0.001\n",
      "[13,  3600] loss: 0.000\n",
      "[13,  3800] loss: 0.002\n",
      "[13,  4000] loss: 0.000\n",
      "[13,  4200] loss: 0.001\n",
      "[13,  4400] loss: 0.000\n",
      "[13,  4600] loss: 0.001\n",
      "[13,  4800] loss: 0.001\n",
      "[13,  5000] loss: 0.006\n",
      "[13,  5200] loss: 0.001\n",
      "[13,  5400] loss: 0.002\n",
      "[13,  5600] loss: 0.000\n",
      "Accuracy : 98.323333 %\n",
      "Training epoch  13\n",
      "[14,   200] loss: 0.000\n",
      "[14,   400] loss: 0.002\n",
      "[14,   600] loss: 0.001\n",
      "[14,   800] loss: 0.000\n",
      "[14,  1000] loss: 0.000\n",
      "[14,  1200] loss: 0.000\n",
      "[14,  1400] loss: 0.000\n",
      "[14,  1600] loss: 0.002\n",
      "[14,  1800] loss: 0.001\n",
      "[14,  2000] loss: 0.000\n",
      "[14,  2200] loss: 0.003\n",
      "[14,  2400] loss: 0.000\n",
      "[14,  2600] loss: 0.001\n",
      "[14,  2800] loss: 0.002\n",
      "[14,  3000] loss: 0.000\n",
      "[14,  3200] loss: 0.000\n",
      "[14,  3400] loss: 0.000\n",
      "[14,  3600] loss: 0.000\n",
      "[14,  3800] loss: 0.001\n",
      "[14,  4000] loss: 0.002\n",
      "[14,  4200] loss: 0.000\n",
      "[14,  4400] loss: 0.001\n",
      "[14,  4600] loss: 0.000\n",
      "[14,  4800] loss: 0.000\n",
      "[14,  5000] loss: 0.001\n",
      "[14,  5200] loss: 0.001\n",
      "[14,  5400] loss: 0.001\n",
      "[14,  5600] loss: 0.000\n",
      "Accuracy : 98.420000 %\n",
      "Training epoch  14\n",
      "[15,   200] loss: 0.002\n",
      "[15,   400] loss: 0.000\n",
      "[15,   600] loss: 0.000\n",
      "[15,   800] loss: 0.000\n",
      "[15,  1000] loss: 0.000\n",
      "[15,  1200] loss: 0.005\n",
      "[15,  1400] loss: 0.001\n",
      "[15,  1600] loss: 0.000\n",
      "[15,  1800] loss: 0.000\n",
      "[15,  2000] loss: 0.001\n",
      "[15,  2200] loss: 0.001\n",
      "[15,  2400] loss: 0.001\n",
      "[15,  2600] loss: 0.001\n",
      "[15,  2800] loss: 0.005\n",
      "[15,  3000] loss: 0.001\n",
      "[15,  3200] loss: 0.000\n",
      "[15,  3400] loss: 0.001\n",
      "[15,  3600] loss: 0.000\n",
      "[15,  3800] loss: 0.000\n",
      "[15,  4000] loss: 0.000\n",
      "[15,  4200] loss: 0.002\n",
      "[15,  4400] loss: 0.000\n",
      "[15,  4600] loss: 0.000\n",
      "[15,  4800] loss: 0.005\n",
      "[15,  5000] loss: 0.001\n",
      "[15,  5200] loss: 0.000\n",
      "[15,  5400] loss: 0.001\n",
      "[15,  5600] loss: 0.000\n",
      "Accuracy : 98.296667 %\n",
      "Training epoch  15\n",
      "[16,   200] loss: 0.000\n",
      "[16,   400] loss: 0.001\n",
      "[16,   600] loss: 0.000\n",
      "[16,   800] loss: 0.000\n",
      "[16,  1000] loss: 0.000\n",
      "[16,  1200] loss: 0.001\n",
      "[16,  1400] loss: 0.000\n",
      "[16,  1600] loss: 0.000\n",
      "[16,  1800] loss: 0.000\n",
      "[16,  2000] loss: 0.000\n",
      "[16,  2200] loss: 0.000\n",
      "[16,  2400] loss: 0.000\n",
      "[16,  2600] loss: 0.001\n",
      "[16,  2800] loss: 0.000\n",
      "[16,  3000] loss: 0.000\n",
      "[16,  3200] loss: 0.000\n",
      "[16,  3400] loss: 0.000\n",
      "[16,  3600] loss: 0.000\n",
      "[16,  3800] loss: 0.000\n",
      "[16,  4000] loss: 0.001\n",
      "[16,  4200] loss: 0.000\n",
      "[16,  4400] loss: 0.001\n",
      "[16,  4600] loss: 0.000\n",
      "[16,  4800] loss: 0.000\n",
      "[16,  5000] loss: 0.002\n",
      "[16,  5200] loss: 0.001\n",
      "[16,  5400] loss: 0.002\n",
      "[16,  5600] loss: 0.000\n",
      "Accuracy : 98.450000 %\n",
      "Training epoch  16\n",
      "[17,   200] loss: 0.003\n",
      "[17,   400] loss: 0.001\n",
      "[17,   600] loss: 0.001\n",
      "[17,   800] loss: 0.002\n",
      "[17,  1000] loss: 0.001\n",
      "[17,  1200] loss: 0.000\n",
      "[17,  1400] loss: 0.001\n",
      "[17,  1600] loss: 0.000\n",
      "[17,  1800] loss: 0.000\n",
      "[17,  2000] loss: 0.000\n",
      "[17,  2200] loss: 0.000\n",
      "[17,  2400] loss: 0.000\n",
      "[17,  2600] loss: 0.001\n",
      "[17,  2800] loss: 0.000\n",
      "[17,  3000] loss: 0.001\n",
      "[17,  3200] loss: 0.000\n",
      "[17,  3400] loss: 0.000\n",
      "[17,  3600] loss: 0.001\n",
      "[17,  3800] loss: 0.000\n",
      "[17,  4000] loss: 0.002\n",
      "[17,  4200] loss: 0.000\n",
      "[17,  4400] loss: 0.000\n",
      "[17,  4600] loss: 0.002\n",
      "[17,  4800] loss: 0.000\n",
      "[17,  5000] loss: 0.000\n",
      "[17,  5200] loss: 0.000\n",
      "[17,  5400] loss: 0.000\n",
      "[17,  5600] loss: 0.000\n",
      "Accuracy : 98.516667 %\n",
      "Training epoch  17\n",
      "[18,   200] loss: 0.000\n",
      "[18,   400] loss: 0.003\n",
      "[18,   600] loss: 0.001\n",
      "[18,   800] loss: 0.003\n",
      "[18,  1000] loss: 0.001\n",
      "[18,  1200] loss: 0.001\n",
      "[18,  1400] loss: 0.002\n",
      "[18,  1600] loss: 0.000\n",
      "[18,  1800] loss: 0.000\n",
      "[18,  2000] loss: 0.000\n",
      "[18,  2200] loss: 0.000\n",
      "[18,  2400] loss: 0.003\n",
      "[18,  2600] loss: 0.000\n",
      "[18,  2800] loss: 0.000\n",
      "[18,  3000] loss: 0.002\n",
      "[18,  3200] loss: 0.001\n",
      "[18,  3400] loss: 0.000\n",
      "[18,  3600] loss: 0.001\n",
      "[18,  3800] loss: 0.000\n",
      "[18,  4000] loss: 0.000\n",
      "[18,  4200] loss: 0.002\n",
      "[18,  4400] loss: 0.000\n",
      "[18,  4600] loss: 0.000\n",
      "[18,  4800] loss: 0.000\n",
      "[18,  5000] loss: 0.000\n",
      "[18,  5200] loss: 0.000\n",
      "[18,  5400] loss: 0.002\n",
      "[18,  5600] loss: 0.002\n",
      "Accuracy : 98.460000 %\n",
      "Training epoch  18\n",
      "[19,   200] loss: 0.002\n",
      "[19,   400] loss: 0.000\n",
      "[19,   600] loss: 0.001\n",
      "[19,   800] loss: 0.000\n",
      "[19,  1000] loss: 0.006\n",
      "[19,  1200] loss: 0.000\n",
      "[19,  1400] loss: 0.004\n",
      "[19,  1600] loss: 0.000\n",
      "[19,  1800] loss: 0.000\n",
      "[19,  2000] loss: 0.000\n",
      "[19,  2200] loss: 0.000\n",
      "[19,  2400] loss: 0.001\n",
      "[19,  2600] loss: 0.000\n",
      "[19,  2800] loss: 0.000\n",
      "[19,  3000] loss: 0.000\n",
      "[19,  3200] loss: 0.000\n",
      "[19,  3400] loss: 0.000\n",
      "[19,  3600] loss: 0.000\n",
      "[19,  3800] loss: 0.000\n",
      "[19,  4000] loss: 0.000\n",
      "[19,  4200] loss: 0.001\n",
      "[19,  4400] loss: 0.000\n",
      "[19,  4600] loss: 0.000\n",
      "[19,  4800] loss: 0.002\n",
      "[19,  5000] loss: 0.000\n",
      "[19,  5200] loss: 0.001\n",
      "[19,  5400] loss: 0.000\n",
      "[19,  5600] loss: 0.000\n",
      "Accuracy : 98.310000 %\n",
      "Training epoch  19\n",
      "[20,   200] loss: 0.000\n",
      "[20,   400] loss: 0.000\n",
      "[20,   600] loss: 0.000\n",
      "[20,   800] loss: 0.000\n",
      "[20,  1000] loss: 0.000\n",
      "[20,  1200] loss: 0.000\n",
      "[20,  1400] loss: 0.001\n",
      "[20,  1600] loss: 0.000\n",
      "[20,  1800] loss: 0.000\n",
      "[20,  2000] loss: 0.001\n",
      "[20,  2200] loss: 0.001\n",
      "[20,  2400] loss: 0.000\n",
      "[20,  2600] loss: 0.000\n",
      "[20,  2800] loss: 0.000\n",
      "[20,  3000] loss: 0.000\n",
      "[20,  3200] loss: 0.000\n",
      "[20,  3400] loss: 0.000\n",
      "[20,  3600] loss: 0.000\n",
      "[20,  3800] loss: 0.000\n",
      "[20,  4000] loss: 0.002\n",
      "[20,  4200] loss: 0.000\n",
      "[20,  4400] loss: 0.002\n",
      "[20,  4600] loss: 0.002\n",
      "[20,  4800] loss: 0.002\n",
      "[20,  5000] loss: 0.000\n",
      "[20,  5200] loss: 0.002\n",
      "[20,  5400] loss: 0.000\n",
      "[20,  5600] loss: 0.000\n",
      "Accuracy : 98.320000 %\n",
      "Training epoch  20\n",
      "[21,   200] loss: 0.000\n",
      "[21,   400] loss: 0.000\n",
      "[21,   600] loss: 0.001\n",
      "[21,   800] loss: 0.000\n",
      "[21,  1000] loss: 0.000\n",
      "[21,  1200] loss: 0.001\n",
      "[21,  1400] loss: 0.000\n",
      "[21,  1600] loss: 0.001\n",
      "[21,  1800] loss: 0.000\n",
      "[21,  2000] loss: 0.000\n",
      "[21,  2200] loss: 0.000\n",
      "[21,  2400] loss: 0.001\n",
      "[21,  2600] loss: 0.000\n",
      "[21,  2800] loss: 0.000\n",
      "[21,  3000] loss: 0.000\n",
      "[21,  3200] loss: 0.000\n",
      "[21,  3400] loss: 0.002\n",
      "[21,  3600] loss: 0.000\n",
      "[21,  3800] loss: 0.000\n",
      "[21,  4000] loss: 0.000\n",
      "[21,  4200] loss: 0.000\n",
      "[21,  4400] loss: 0.000\n",
      "[21,  4600] loss: 0.000\n",
      "[21,  4800] loss: 0.000\n",
      "[21,  5000] loss: 0.000\n",
      "[21,  5200] loss: 0.000\n",
      "[21,  5400] loss: 0.000\n",
      "[21,  5600] loss: 0.000\n",
      "Accuracy : 98.380000 %\n",
      "Training epoch  21\n",
      "[22,   200] loss: 0.000\n",
      "[22,   400] loss: 0.000\n",
      "[22,   600] loss: 0.000\n",
      "[22,   800] loss: 0.000\n",
      "[22,  1000] loss: 0.000\n",
      "[22,  1200] loss: 0.000\n",
      "[22,  1400] loss: 0.000\n",
      "[22,  1600] loss: 0.001\n",
      "[22,  1800] loss: 0.000\n",
      "[22,  2000] loss: 0.000\n",
      "[22,  2200] loss: 0.000\n",
      "[22,  2400] loss: 0.000\n",
      "[22,  2600] loss: 0.000\n",
      "[22,  2800] loss: 0.000\n",
      "[22,  3000] loss: 0.000\n",
      "[22,  3200] loss: 0.002\n",
      "[22,  3400] loss: 0.000\n",
      "[22,  3600] loss: 0.000\n",
      "[22,  3800] loss: 0.000\n",
      "[22,  4000] loss: 0.001\n",
      "[22,  4200] loss: 0.000\n",
      "[22,  4400] loss: 0.000\n",
      "[22,  4600] loss: 0.002\n",
      "[22,  4800] loss: 0.000\n",
      "[22,  5000] loss: 0.001\n",
      "[22,  5200] loss: 0.000\n",
      "[22,  5400] loss: 0.000\n",
      "[22,  5600] loss: 0.000\n",
      "Accuracy : 98.410000 %\n",
      "Training epoch  22\n",
      "[23,   200] loss: 0.000\n",
      "[23,   400] loss: 0.000\n",
      "[23,   600] loss: 0.000\n",
      "[23,   800] loss: 0.000\n",
      "[23,  1000] loss: 0.000\n",
      "[23,  1200] loss: 0.001\n",
      "[23,  1400] loss: 0.000\n",
      "[23,  1600] loss: 0.000\n",
      "[23,  1800] loss: 0.000\n",
      "[23,  2000] loss: 0.000\n",
      "[23,  2200] loss: 0.000\n",
      "[23,  2400] loss: 0.000\n",
      "[23,  2600] loss: 0.000\n",
      "[23,  2800] loss: 0.000\n",
      "[23,  3000] loss: 0.000\n",
      "[23,  3200] loss: 0.000\n",
      "[23,  3400] loss: 0.000\n",
      "[23,  3600] loss: 0.000\n",
      "[23,  3800] loss: 0.000\n",
      "[23,  4000] loss: 0.000\n",
      "[23,  4200] loss: 0.000\n",
      "[23,  4400] loss: 0.000\n",
      "[23,  4600] loss: 0.001\n",
      "[23,  4800] loss: 0.000\n",
      "[23,  5000] loss: 0.000\n",
      "[23,  5200] loss: 0.002\n",
      "[23,  5400] loss: 0.000\n",
      "[23,  5600] loss: 0.001\n",
      "Accuracy : 98.453333 %\n",
      "Training epoch  23\n",
      "[24,   200] loss: 0.000\n",
      "[24,   400] loss: 0.000\n",
      "[24,   600] loss: 0.000\n",
      "[24,   800] loss: 0.000\n",
      "[24,  1000] loss: 0.000\n",
      "[24,  1200] loss: 0.000\n",
      "[24,  1400] loss: 0.000\n",
      "[24,  1600] loss: 0.000\n",
      "[24,  1800] loss: 0.001\n",
      "[24,  2000] loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24,  2200] loss: 0.000\n",
      "[24,  2400] loss: 0.002\n",
      "[24,  2600] loss: 0.000\n",
      "[24,  2800] loss: 0.000\n",
      "[24,  3000] loss: 0.000\n",
      "[24,  3200] loss: 0.000\n",
      "[24,  3400] loss: 0.000\n",
      "[24,  3600] loss: 0.000\n",
      "[24,  3800] loss: 0.000\n",
      "[24,  4000] loss: 0.000\n",
      "[24,  4200] loss: 0.000\n",
      "[24,  4400] loss: 0.000\n",
      "[24,  4600] loss: 0.000\n",
      "[24,  4800] loss: 0.000\n",
      "[24,  5000] loss: 0.000\n",
      "[24,  5200] loss: 0.000\n",
      "[24,  5400] loss: 0.000\n",
      "[24,  5600] loss: 0.000\n",
      "Accuracy : 98.513333 %\n",
      "Training epoch  24\n",
      "[25,   200] loss: 0.000\n",
      "[25,   400] loss: 0.000\n",
      "[25,   600] loss: 0.000\n",
      "[25,   800] loss: 0.002\n",
      "[25,  1000] loss: 0.000\n",
      "[25,  1200] loss: 0.000\n",
      "[25,  1400] loss: 0.000\n",
      "[25,  1600] loss: 0.000\n",
      "[25,  1800] loss: 0.000\n",
      "[25,  2000] loss: 0.000\n",
      "[25,  2200] loss: 0.000\n",
      "[25,  2400] loss: 0.000\n",
      "[25,  2600] loss: 0.000\n",
      "[25,  2800] loss: 0.000\n",
      "[25,  3000] loss: 0.000\n",
      "[25,  3200] loss: 0.001\n",
      "[25,  3400] loss: 0.000\n",
      "[25,  3600] loss: 0.000\n",
      "[25,  3800] loss: 0.000\n",
      "[25,  4000] loss: 0.000\n",
      "[25,  4200] loss: 0.000\n",
      "[25,  4400] loss: 0.000\n",
      "[25,  4600] loss: 0.000\n",
      "[25,  4800] loss: 0.000\n",
      "[25,  5000] loss: 0.001\n",
      "[25,  5200] loss: 0.000\n",
      "[25,  5400] loss: 0.000\n",
      "[25,  5600] loss: 0.000\n",
      "Accuracy : 98.506667 %\n",
      "Training epoch  25\n",
      "[26,   200] loss: 0.000\n",
      "[26,   400] loss: 0.002\n",
      "[26,   600] loss: 0.000\n",
      "[26,   800] loss: 0.000\n",
      "[26,  1000] loss: 0.000\n",
      "[26,  1200] loss: 0.000\n",
      "[26,  1400] loss: 0.000\n",
      "[26,  1600] loss: 0.000\n",
      "[26,  1800] loss: 0.000\n",
      "[26,  2000] loss: 0.000\n",
      "[26,  2200] loss: 0.000\n",
      "[26,  2400] loss: 0.000\n",
      "[26,  2600] loss: 0.000\n",
      "[26,  2800] loss: 0.000\n",
      "[26,  3000] loss: 0.000\n",
      "[26,  3200] loss: 0.000\n",
      "[26,  3400] loss: 0.000\n",
      "[26,  3600] loss: 0.000\n",
      "[26,  3800] loss: 0.000\n",
      "[26,  4000] loss: 0.000\n",
      "[26,  4200] loss: 0.000\n",
      "[26,  4400] loss: 0.000\n",
      "[26,  4600] loss: 0.000\n",
      "[26,  4800] loss: 0.000\n",
      "[26,  5000] loss: 0.001\n",
      "[26,  5200] loss: 0.000\n",
      "[26,  5400] loss: 0.000\n",
      "[26,  5600] loss: 0.000\n",
      "Accuracy : 98.533333 %\n",
      "Training epoch  26\n",
      "[27,   200] loss: 0.000\n",
      "[27,   400] loss: 0.000\n",
      "[27,   600] loss: 0.001\n",
      "[27,   800] loss: 0.000\n",
      "[27,  1000] loss: 0.000\n",
      "[27,  1200] loss: 0.000\n",
      "[27,  1400] loss: 0.000\n",
      "[27,  1600] loss: 0.000\n",
      "[27,  1800] loss: 0.000\n",
      "[27,  2000] loss: 0.000\n",
      "[27,  2200] loss: 0.000\n",
      "[27,  2400] loss: 0.000\n",
      "[27,  2600] loss: 0.000\n",
      "[27,  2800] loss: 0.000\n",
      "[27,  3000] loss: 0.000\n",
      "[27,  3200] loss: 0.000\n",
      "[27,  3400] loss: 0.000\n",
      "[27,  3600] loss: 0.000\n",
      "[27,  3800] loss: 0.000\n",
      "[27,  4000] loss: 0.000\n",
      "[27,  4200] loss: 0.000\n",
      "[27,  4400] loss: 0.000\n",
      "[27,  4600] loss: 0.000\n",
      "[27,  4800] loss: 0.000\n",
      "[27,  5000] loss: 0.000\n",
      "[27,  5200] loss: 0.000\n",
      "[27,  5400] loss: 0.000\n",
      "[27,  5600] loss: 0.000\n",
      "Accuracy : 98.523333 %\n",
      "Training epoch  27\n",
      "[28,   200] loss: 0.000\n",
      "[28,   400] loss: 0.000\n",
      "[28,   600] loss: 0.000\n",
      "[28,   800] loss: 0.001\n",
      "[28,  1000] loss: 0.000\n",
      "[28,  1200] loss: 0.000\n",
      "[28,  1400] loss: 0.000\n",
      "[28,  1600] loss: 0.001\n",
      "[28,  1800] loss: 0.000\n",
      "[28,  2000] loss: 0.000\n",
      "[28,  2200] loss: 0.000\n",
      "[28,  2400] loss: 0.000\n",
      "[28,  2600] loss: 0.000\n",
      "[28,  2800] loss: 0.000\n",
      "[28,  3000] loss: 0.001\n",
      "[28,  3200] loss: 0.000\n",
      "[28,  3400] loss: 0.000\n",
      "[28,  3600] loss: 0.000\n",
      "[28,  3800] loss: 0.000\n",
      "[28,  4000] loss: 0.000\n",
      "[28,  4200] loss: 0.000\n",
      "[28,  4400] loss: 0.000\n",
      "[28,  4600] loss: 0.000\n",
      "[28,  4800] loss: 0.000\n",
      "[28,  5000] loss: 0.000\n",
      "[28,  5200] loss: 0.000\n",
      "[28,  5400] loss: 0.000\n",
      "[28,  5600] loss: 0.000\n",
      "Accuracy : 98.583333 %\n",
      "Training epoch  28\n",
      "[29,   200] loss: 0.000\n",
      "[29,   400] loss: 0.000\n",
      "[29,   600] loss: 0.000\n",
      "[29,   800] loss: 0.000\n",
      "[29,  1000] loss: 0.000\n",
      "[29,  1200] loss: 0.000\n",
      "[29,  1400] loss: 0.000\n",
      "[29,  1600] loss: 0.000\n",
      "[29,  1800] loss: 0.000\n",
      "[29,  2000] loss: 0.000\n",
      "[29,  2200] loss: 0.000\n",
      "[29,  2400] loss: 0.000\n",
      "[29,  2600] loss: 0.000\n",
      "[29,  2800] loss: 0.000\n",
      "[29,  3000] loss: 0.000\n",
      "[29,  3200] loss: 0.000\n",
      "[29,  3400] loss: 0.000\n",
      "[29,  3600] loss: 0.000\n",
      "[29,  3800] loss: 0.000\n",
      "[29,  4000] loss: 0.000\n",
      "[29,  4200] loss: 0.000\n",
      "[29,  4400] loss: 0.000\n",
      "[29,  4600] loss: 0.000\n",
      "[29,  4800] loss: 0.002\n",
      "[29,  5000] loss: 0.000\n",
      "[29,  5200] loss: 0.000\n",
      "[29,  5400] loss: 0.000\n",
      "[29,  5600] loss: 0.000\n",
      "Accuracy : 98.513333 %\n",
      "Training epoch  29\n",
      "[30,   200] loss: 0.000\n",
      "[30,   400] loss: 0.000\n",
      "[30,   600] loss: 0.000\n",
      "[30,   800] loss: 0.000\n",
      "[30,  1000] loss: 0.000\n",
      "[30,  1200] loss: 0.000\n",
      "[30,  1400] loss: 0.000\n",
      "[30,  1600] loss: 0.000\n",
      "[30,  1800] loss: 0.000\n",
      "[30,  2000] loss: 0.000\n",
      "[30,  2200] loss: 0.000\n",
      "[30,  2400] loss: 0.000\n",
      "[30,  2600] loss: 0.000\n",
      "[30,  2800] loss: 0.000\n",
      "[30,  3000] loss: 0.000\n",
      "[30,  3200] loss: 0.000\n",
      "[30,  3400] loss: 0.000\n",
      "[30,  3600] loss: 0.000\n",
      "[30,  3800] loss: 0.000\n",
      "[30,  4000] loss: 0.000\n",
      "[30,  4200] loss: 0.000\n",
      "[30,  4400] loss: 0.000\n",
      "[30,  4600] loss: 0.000\n",
      "[30,  4800] loss: 0.000\n",
      "[30,  5000] loss: 0.000\n",
      "[30,  5200] loss: 0.000\n",
      "[30,  5400] loss: 0.000\n",
      "[30,  5600] loss: 0.000\n",
      "Accuracy : 98.530000 %\n",
      "Training epoch  30\n",
      "[31,   200] loss: 0.000\n",
      "[31,   400] loss: 0.000\n",
      "[31,   600] loss: 0.001\n",
      "[31,   800] loss: 0.000\n",
      "[31,  1000] loss: 0.000\n",
      "[31,  1200] loss: 0.000\n",
      "[31,  1400] loss: 0.000\n",
      "[31,  1600] loss: 0.000\n",
      "[31,  1800] loss: 0.000\n",
      "[31,  2000] loss: 0.000\n",
      "[31,  2200] loss: 0.000\n",
      "[31,  2400] loss: 0.000\n",
      "[31,  2600] loss: 0.001\n",
      "[31,  2800] loss: 0.000\n",
      "[31,  3000] loss: 0.001\n",
      "[31,  3200] loss: 0.000\n",
      "[31,  3400] loss: 0.000\n",
      "[31,  3600] loss: 0.000\n",
      "[31,  3800] loss: 0.000\n",
      "[31,  4000] loss: 0.000\n",
      "[31,  4200] loss: 0.000\n",
      "[31,  4400] loss: 0.001\n",
      "[31,  4600] loss: 0.000\n",
      "[31,  4800] loss: 0.000\n",
      "[31,  5000] loss: 0.000\n",
      "[31,  5200] loss: 0.000\n",
      "[31,  5400] loss: 0.000\n",
      "[31,  5600] loss: 0.000\n",
      "Accuracy : 98.486667 %\n",
      "Training epoch  31\n",
      "[32,   200] loss: 0.000\n",
      "[32,   400] loss: 0.000\n",
      "[32,   600] loss: 0.000\n",
      "[32,   800] loss: 0.000\n",
      "[32,  1000] loss: 0.000\n",
      "[32,  1200] loss: 0.000\n",
      "[32,  1400] loss: 0.000\n",
      "[32,  1600] loss: 0.000\n",
      "[32,  1800] loss: 0.000\n",
      "[32,  2000] loss: 0.000\n",
      "[32,  2200] loss: 0.000\n",
      "[32,  2400] loss: 0.000\n",
      "[32,  2600] loss: 0.000\n",
      "[32,  2800] loss: 0.000\n",
      "[32,  3000] loss: 0.000\n",
      "[32,  3200] loss: 0.000\n",
      "[32,  3400] loss: 0.000\n",
      "[32,  3600] loss: 0.000\n",
      "[32,  3800] loss: 0.000\n",
      "[32,  4000] loss: 0.000\n",
      "[32,  4200] loss: 0.000\n",
      "[32,  4400] loss: 0.000\n",
      "[32,  4600] loss: 0.000\n",
      "[32,  4800] loss: 0.000\n",
      "[32,  5000] loss: 0.000\n",
      "[32,  5200] loss: 0.000\n",
      "[32,  5400] loss: 0.000\n",
      "[32,  5600] loss: 0.000\n",
      "Accuracy : 98.533333 %\n",
      "Training epoch  32\n",
      "[33,   200] loss: 0.000\n",
      "[33,   400] loss: 0.000\n",
      "[33,   600] loss: 0.000\n",
      "[33,   800] loss: 0.000\n",
      "[33,  1000] loss: 0.000\n",
      "[33,  1200] loss: 0.000\n",
      "[33,  1400] loss: 0.000\n",
      "[33,  1600] loss: 0.000\n",
      "[33,  1800] loss: 0.000\n",
      "[33,  2000] loss: 0.000\n",
      "[33,  2200] loss: 0.000\n",
      "[33,  2400] loss: 0.000\n",
      "[33,  2600] loss: 0.000\n",
      "[33,  2800] loss: 0.000\n",
      "[33,  3000] loss: 0.000\n",
      "[33,  3200] loss: 0.000\n",
      "[33,  3400] loss: 0.000\n",
      "[33,  3600] loss: 0.000\n",
      "[33,  3800] loss: 0.000\n",
      "[33,  4000] loss: 0.000\n",
      "[33,  4200] loss: 0.000\n",
      "[33,  4400] loss: 0.000\n",
      "[33,  4600] loss: 0.000\n",
      "[33,  4800] loss: 0.000\n",
      "[33,  5000] loss: 0.000\n",
      "[33,  5200] loss: 0.000\n",
      "[33,  5400] loss: 0.000\n",
      "[33,  5600] loss: 0.000\n",
      "Accuracy : 98.573333 %\n",
      "Training epoch  33\n",
      "[34,   200] loss: 0.000\n",
      "[34,   400] loss: 0.000\n",
      "[34,   600] loss: 0.001\n",
      "[34,   800] loss: 0.000\n",
      "[34,  1000] loss: 0.000\n",
      "[34,  1200] loss: 0.000\n",
      "[34,  1400] loss: 0.000\n",
      "[34,  1600] loss: 0.000\n",
      "[34,  1800] loss: 0.000\n",
      "[34,  2000] loss: 0.000\n",
      "[34,  2200] loss: 0.000\n",
      "[34,  2400] loss: 0.000\n",
      "[34,  2600] loss: 0.000\n",
      "[34,  2800] loss: 0.000\n",
      "[34,  3000] loss: 0.000\n",
      "[34,  3200] loss: 0.000\n",
      "[34,  3400] loss: 0.000\n",
      "[34,  3600] loss: 0.000\n",
      "[34,  3800] loss: 0.000\n",
      "[34,  4000] loss: 0.000\n",
      "[34,  4200] loss: 0.000\n",
      "[34,  4400] loss: 0.000\n",
      "[34,  4600] loss: 0.000\n",
      "[34,  4800] loss: 0.000\n",
      "[34,  5000] loss: 0.000\n",
      "[34,  5200] loss: 0.000\n",
      "[34,  5400] loss: 0.000\n",
      "[34,  5600] loss: 0.000\n",
      "Accuracy : 98.566667 %\n",
      "Training epoch  34\n",
      "[35,   200] loss: 0.000\n",
      "[35,   400] loss: 0.000\n",
      "[35,   600] loss: 0.000\n",
      "[35,   800] loss: 0.000\n",
      "[35,  1000] loss: 0.000\n",
      "[35,  1200] loss: 0.000\n",
      "[35,  1400] loss: 0.000\n",
      "[35,  1600] loss: 0.000\n",
      "[35,  1800] loss: 0.000\n",
      "[35,  2000] loss: 0.000\n",
      "[35,  2200] loss: 0.000\n",
      "[35,  2400] loss: 0.000\n",
      "[35,  2600] loss: 0.000\n",
      "[35,  2800] loss: 0.000\n",
      "[35,  3000] loss: 0.000\n",
      "[35,  3200] loss: 0.001\n",
      "[35,  3400] loss: 0.000\n",
      "[35,  3600] loss: 0.000\n",
      "[35,  3800] loss: 0.000\n",
      "[35,  4000] loss: 0.000\n",
      "[35,  4200] loss: 0.000\n",
      "[35,  4400] loss: 0.000\n",
      "[35,  4600] loss: 0.000\n",
      "[35,  4800] loss: 0.000\n",
      "[35,  5000] loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35,  5200] loss: 0.000\n",
      "[35,  5400] loss: 0.000\n",
      "[35,  5600] loss: 0.000\n",
      "Accuracy : 98.543333 %\n",
      "Training epoch  35\n",
      "[36,   200] loss: 0.000\n",
      "[36,   400] loss: 0.000\n",
      "[36,   600] loss: 0.000\n",
      "[36,   800] loss: 0.000\n",
      "[36,  1000] loss: 0.000\n",
      "[36,  1200] loss: 0.000\n",
      "[36,  1400] loss: 0.000\n",
      "[36,  1600] loss: 0.000\n",
      "[36,  1800] loss: 0.000\n",
      "[36,  2000] loss: 0.000\n",
      "[36,  2200] loss: 0.000\n",
      "[36,  2400] loss: 0.000\n",
      "[36,  2600] loss: 0.000\n",
      "[36,  2800] loss: 0.000\n",
      "[36,  3000] loss: 0.000\n",
      "[36,  3200] loss: 0.000\n",
      "[36,  3400] loss: 0.000\n",
      "[36,  3600] loss: 0.000\n",
      "[36,  3800] loss: 0.000\n",
      "[36,  4000] loss: 0.000\n",
      "[36,  4200] loss: 0.000\n",
      "[36,  4400] loss: 0.000\n",
      "[36,  4600] loss: 0.000\n",
      "[36,  4800] loss: 0.000\n",
      "[36,  5000] loss: 0.000\n",
      "[36,  5200] loss: 0.000\n",
      "[36,  5400] loss: 0.000\n",
      "[36,  5600] loss: 0.000\n",
      "Accuracy : 98.606667 %\n",
      "Training epoch  36\n",
      "[37,   200] loss: 0.000\n",
      "[37,   400] loss: 0.000\n",
      "[37,   600] loss: 0.000\n",
      "[37,   800] loss: 0.000\n",
      "[37,  1000] loss: 0.000\n",
      "[37,  1200] loss: 0.000\n",
      "[37,  1400] loss: 0.000\n",
      "[37,  1600] loss: 0.000\n",
      "[37,  1800] loss: 0.000\n",
      "[37,  2000] loss: 0.000\n",
      "[37,  2200] loss: 0.000\n",
      "[37,  2400] loss: 0.000\n",
      "[37,  2600] loss: 0.000\n",
      "[37,  2800] loss: 0.000\n",
      "[37,  3000] loss: 0.000\n",
      "[37,  3200] loss: 0.000\n",
      "[37,  3400] loss: 0.000\n",
      "[37,  3600] loss: 0.000\n",
      "[37,  3800] loss: 0.000\n",
      "[37,  4000] loss: 0.000\n",
      "[37,  4200] loss: 0.000\n",
      "[37,  4400] loss: 0.000\n",
      "[37,  4600] loss: 0.000\n",
      "[37,  4800] loss: 0.000\n",
      "[37,  5000] loss: 0.000\n",
      "[37,  5200] loss: 0.000\n",
      "[37,  5400] loss: 0.000\n",
      "[37,  5600] loss: 0.000\n",
      "Accuracy : 98.553333 %\n",
      "Training epoch  37\n",
      "[38,   200] loss: 0.000\n",
      "[38,   400] loss: 0.000\n",
      "[38,   600] loss: 0.000\n",
      "[38,   800] loss: 0.000\n",
      "[38,  1000] loss: 0.000\n",
      "[38,  1200] loss: 0.000\n",
      "[38,  1400] loss: 0.000\n",
      "[38,  1600] loss: 0.000\n",
      "[38,  1800] loss: 0.000\n",
      "[38,  2000] loss: 0.000\n",
      "[38,  2200] loss: 0.000\n",
      "[38,  2400] loss: 0.000\n",
      "[38,  2600] loss: 0.000\n",
      "[38,  2800] loss: 0.000\n",
      "[38,  3000] loss: 0.000\n",
      "[38,  3200] loss: 0.000\n",
      "[38,  3400] loss: 0.000\n",
      "[38,  3600] loss: 0.000\n",
      "[38,  3800] loss: 0.000\n",
      "[38,  4000] loss: 0.000\n",
      "[38,  4200] loss: 0.000\n",
      "[38,  4400] loss: 0.000\n",
      "[38,  4600] loss: 0.000\n",
      "[38,  4800] loss: 0.000\n",
      "[38,  5000] loss: 0.000\n",
      "[38,  5200] loss: 0.000\n",
      "[38,  5400] loss: 0.000\n",
      "[38,  5600] loss: 0.000\n",
      "Accuracy : 98.526667 %\n",
      "Training epoch  38\n",
      "[39,   200] loss: 0.000\n",
      "[39,   400] loss: 0.000\n",
      "[39,   600] loss: 0.000\n",
      "[39,   800] loss: 0.000\n",
      "[39,  1000] loss: 0.000\n",
      "[39,  1200] loss: 0.000\n",
      "[39,  1400] loss: 0.000\n",
      "[39,  1600] loss: 0.000\n",
      "[39,  1800] loss: 0.000\n",
      "[39,  2000] loss: 0.000\n",
      "[39,  2200] loss: 0.000\n",
      "[39,  2400] loss: 0.000\n",
      "[39,  2600] loss: 0.000\n",
      "[39,  2800] loss: 0.000\n",
      "[39,  3000] loss: 0.000\n",
      "[39,  3200] loss: 0.000\n",
      "[39,  3400] loss: 0.000\n",
      "[39,  3600] loss: 0.000\n",
      "[39,  3800] loss: 0.000\n",
      "[39,  4000] loss: 0.000\n",
      "[39,  4200] loss: 0.000\n",
      "[39,  4400] loss: 0.000\n",
      "[39,  4600] loss: 0.000\n",
      "[39,  4800] loss: 0.000\n",
      "[39,  5000] loss: 0.001\n",
      "[39,  5200] loss: 0.000\n",
      "[39,  5400] loss: 0.000\n",
      "[39,  5600] loss: 0.000\n",
      "Accuracy : 98.513333 %\n",
      "Training epoch  39\n",
      "[40,   200] loss: 0.000\n",
      "[40,   400] loss: 0.000\n",
      "[40,   600] loss: 0.000\n",
      "[40,   800] loss: 0.000\n",
      "[40,  1000] loss: 0.000\n",
      "[40,  1200] loss: 0.000\n",
      "[40,  1400] loss: 0.000\n",
      "[40,  1600] loss: 0.000\n",
      "[40,  1800] loss: 0.000\n",
      "[40,  2000] loss: 0.000\n",
      "[40,  2200] loss: 0.000\n",
      "[40,  2400] loss: 0.000\n",
      "[40,  2600] loss: 0.000\n",
      "[40,  2800] loss: 0.000\n",
      "[40,  3000] loss: 0.000\n",
      "[40,  3200] loss: 0.000\n",
      "[40,  3400] loss: 0.000\n",
      "[40,  3600] loss: 0.000\n",
      "[40,  3800] loss: 0.000\n",
      "[40,  4000] loss: 0.000\n",
      "[40,  4200] loss: 0.000\n",
      "[40,  4400] loss: 0.000\n",
      "[40,  4600] loss: 0.000\n",
      "[40,  4800] loss: 0.000\n",
      "[40,  5000] loss: 0.000\n",
      "[40,  5200] loss: 0.000\n",
      "[40,  5400] loss: 0.000\n",
      "[40,  5600] loss: 0.000\n",
      "Accuracy : 98.503333 %\n",
      "Training epoch  40\n",
      "[41,   200] loss: 0.000\n",
      "[41,   400] loss: 0.000\n",
      "[41,   600] loss: 0.000\n",
      "[41,   800] loss: 0.000\n",
      "[41,  1000] loss: 0.000\n",
      "[41,  1200] loss: 0.000\n",
      "[41,  1400] loss: 0.000\n",
      "[41,  1600] loss: 0.000\n",
      "[41,  1800] loss: 0.000\n",
      "[41,  2000] loss: 0.000\n",
      "[41,  2200] loss: 0.000\n",
      "[41,  2400] loss: 0.000\n",
      "[41,  2600] loss: 0.000\n",
      "[41,  2800] loss: 0.000\n",
      "[41,  3000] loss: 0.000\n",
      "[41,  3200] loss: 0.000\n",
      "[41,  3400] loss: 0.000\n",
      "[41,  3600] loss: 0.000\n",
      "[41,  3800] loss: 0.000\n",
      "[41,  4000] loss: 0.000\n",
      "[41,  4200] loss: 0.000\n",
      "[41,  4400] loss: 0.000\n",
      "[41,  4600] loss: 0.000\n",
      "[41,  4800] loss: 0.000\n",
      "[41,  5000] loss: 0.000\n",
      "[41,  5200] loss: 0.000\n",
      "[41,  5400] loss: 0.000\n",
      "[41,  5600] loss: 0.000\n",
      "Accuracy : 98.483333 %\n",
      "Training epoch  41\n",
      "[42,   200] loss: 0.000\n",
      "[42,   400] loss: 0.000\n",
      "[42,   600] loss: 0.000\n",
      "[42,   800] loss: 0.000\n",
      "[42,  1000] loss: 0.000\n",
      "[42,  1200] loss: 0.000\n",
      "[42,  1400] loss: 0.000\n",
      "[42,  1600] loss: 0.000\n",
      "[42,  1800] loss: 0.000\n",
      "[42,  2000] loss: 0.000\n",
      "[42,  2200] loss: 0.000\n",
      "[42,  2400] loss: 0.000\n",
      "[42,  2600] loss: 0.000\n",
      "[42,  2800] loss: 0.000\n",
      "[42,  3000] loss: 0.000\n",
      "[42,  3200] loss: 0.000\n",
      "[42,  3400] loss: 0.000\n",
      "[42,  3600] loss: 0.000\n",
      "[42,  3800] loss: 0.000\n",
      "[42,  4000] loss: 0.000\n",
      "[42,  4200] loss: 0.000\n",
      "[42,  4400] loss: 0.000\n",
      "[42,  4600] loss: 0.000\n",
      "[42,  4800] loss: 0.000\n",
      "[42,  5000] loss: 0.000\n",
      "[42,  5200] loss: 0.000\n",
      "[42,  5400] loss: 0.000\n",
      "[42,  5600] loss: 0.000\n",
      "Accuracy : 98.533333 %\n",
      "Training epoch  42\n",
      "[43,   200] loss: 0.000\n",
      "[43,   400] loss: 0.000\n",
      "[43,   600] loss: 0.000\n",
      "[43,   800] loss: 0.000\n",
      "[43,  1000] loss: 0.000\n",
      "[43,  1200] loss: 0.000\n",
      "[43,  1400] loss: 0.000\n",
      "[43,  1600] loss: 0.000\n",
      "[43,  1800] loss: 0.000\n",
      "[43,  2000] loss: 0.000\n",
      "[43,  2200] loss: 0.000\n",
      "[43,  2400] loss: 0.000\n",
      "[43,  2600] loss: 0.001\n",
      "[43,  2800] loss: 0.000\n",
      "[43,  3000] loss: 0.000\n",
      "[43,  3200] loss: 0.000\n",
      "[43,  3400] loss: 0.000\n",
      "[43,  3600] loss: 0.000\n",
      "[43,  3800] loss: 0.000\n",
      "[43,  4000] loss: 0.000\n",
      "[43,  4200] loss: 0.000\n",
      "[43,  4400] loss: 0.000\n",
      "[43,  4600] loss: 0.000\n",
      "[43,  4800] loss: 0.000\n",
      "[43,  5000] loss: 0.000\n",
      "[43,  5200] loss: 0.000\n",
      "[43,  5400] loss: 0.000\n",
      "[43,  5600] loss: 0.000\n",
      "Accuracy : 98.613333 %\n",
      "Training epoch  43\n",
      "[44,   200] loss: 0.000\n",
      "[44,   400] loss: 0.000\n",
      "[44,   600] loss: 0.000\n",
      "[44,   800] loss: 0.000\n",
      "[44,  1000] loss: 0.000\n",
      "[44,  1200] loss: 0.000\n",
      "[44,  1400] loss: 0.000\n",
      "[44,  1600] loss: 0.000\n",
      "[44,  1800] loss: 0.000\n",
      "[44,  2000] loss: 0.000\n",
      "[44,  2200] loss: 0.000\n",
      "[44,  2400] loss: 0.000\n",
      "[44,  2600] loss: 0.000\n",
      "[44,  2800] loss: 0.000\n",
      "[44,  3000] loss: 0.000\n",
      "[44,  3200] loss: 0.000\n",
      "[44,  3400] loss: 0.000\n",
      "[44,  3600] loss: 0.000\n",
      "[44,  3800] loss: 0.000\n",
      "[44,  4000] loss: 0.000\n",
      "[44,  4200] loss: 0.000\n",
      "[44,  4400] loss: 0.000\n",
      "[44,  4600] loss: 0.000\n",
      "[44,  4800] loss: 0.000\n",
      "[44,  5000] loss: 0.000\n",
      "[44,  5200] loss: 0.000\n",
      "[44,  5400] loss: 0.000\n",
      "[44,  5600] loss: 0.000\n",
      "Accuracy : 98.610000 %\n",
      "Training epoch  44\n",
      "[45,   200] loss: 0.000\n",
      "[45,   400] loss: 0.000\n",
      "[45,   600] loss: 0.000\n",
      "[45,   800] loss: 0.000\n",
      "[45,  1000] loss: 0.000\n",
      "[45,  1200] loss: 0.000\n",
      "[45,  1400] loss: 0.000\n",
      "[45,  1600] loss: 0.000\n",
      "[45,  1800] loss: 0.000\n",
      "[45,  2000] loss: 0.000\n",
      "[45,  2200] loss: 0.000\n",
      "[45,  2400] loss: 0.000\n",
      "[45,  2600] loss: 0.000\n",
      "[45,  2800] loss: 0.000\n",
      "[45,  3000] loss: 0.000\n",
      "[45,  3200] loss: 0.000\n",
      "[45,  3400] loss: 0.000\n",
      "[45,  3600] loss: 0.000\n",
      "[45,  3800] loss: 0.000\n",
      "[45,  4000] loss: 0.000\n",
      "[45,  4200] loss: 0.001\n",
      "[45,  4400] loss: 0.000\n",
      "[45,  4600] loss: 0.000\n",
      "[45,  4800] loss: 0.000\n",
      "[45,  5000] loss: 0.000\n",
      "[45,  5200] loss: 0.000\n",
      "[45,  5400] loss: 0.000\n",
      "[45,  5600] loss: 0.000\n",
      "Accuracy : 98.543333 %\n",
      "Training epoch  45\n",
      "[46,   200] loss: 0.000\n",
      "[46,   400] loss: 0.000\n",
      "[46,   600] loss: 0.000\n",
      "[46,   800] loss: 0.000\n",
      "[46,  1000] loss: 0.000\n",
      "[46,  1200] loss: 0.000\n",
      "[46,  1400] loss: 0.000\n",
      "[46,  1600] loss: 0.000\n",
      "[46,  1800] loss: 0.000\n",
      "[46,  2000] loss: 0.000\n",
      "[46,  2200] loss: 0.000\n",
      "[46,  2400] loss: 0.000\n",
      "[46,  2600] loss: 0.000\n",
      "[46,  2800] loss: 0.000\n",
      "[46,  3000] loss: 0.000\n",
      "[46,  3200] loss: 0.000\n",
      "[46,  3400] loss: 0.000\n",
      "[46,  3600] loss: 0.000\n",
      "[46,  3800] loss: 0.000\n",
      "[46,  4000] loss: 0.000\n",
      "[46,  4200] loss: 0.000\n",
      "[46,  4400] loss: 0.000\n",
      "[46,  4600] loss: 0.000\n",
      "[46,  4800] loss: 0.000\n",
      "[46,  5000] loss: 0.000\n",
      "[46,  5200] loss: 0.000\n",
      "[46,  5400] loss: 0.000\n",
      "[46,  5600] loss: 0.000\n",
      "Accuracy : 98.566667 %\n",
      "Training epoch  46\n",
      "[47,   200] loss: 0.000\n",
      "[47,   400] loss: 0.000\n",
      "[47,   600] loss: 0.000\n",
      "[47,   800] loss: 0.000\n",
      "[47,  1000] loss: 0.000\n",
      "[47,  1200] loss: 0.000\n",
      "[47,  1400] loss: 0.000\n",
      "[47,  1600] loss: 0.000\n",
      "[47,  1800] loss: 0.000\n",
      "[47,  2000] loss: 0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47,  2200] loss: 0.000\n",
      "[47,  2400] loss: 0.000\n",
      "[47,  2600] loss: 0.000\n",
      "[47,  2800] loss: 0.000\n",
      "[47,  3000] loss: 0.000\n",
      "[47,  3200] loss: 0.000\n",
      "[47,  3400] loss: 0.000\n",
      "[47,  3600] loss: 0.000\n",
      "[47,  3800] loss: 0.000\n",
      "[47,  4000] loss: 0.000\n",
      "[47,  4200] loss: 0.000\n",
      "[47,  4400] loss: 0.000\n",
      "[47,  4600] loss: 0.000\n",
      "[47,  4800] loss: 0.000\n",
      "[47,  5000] loss: 0.000\n",
      "[47,  5200] loss: 0.001\n",
      "[47,  5400] loss: 0.000\n",
      "[47,  5600] loss: 0.000\n",
      "Accuracy : 98.533333 %\n",
      "Training epoch  47\n",
      "[48,   200] loss: 0.000\n",
      "[48,   400] loss: 0.000\n",
      "[48,   600] loss: 0.000\n",
      "[48,   800] loss: 0.000\n",
      "[48,  1000] loss: 0.000\n",
      "[48,  1200] loss: 0.000\n",
      "[48,  1400] loss: 0.000\n",
      "[48,  1600] loss: 0.000\n",
      "[48,  1800] loss: 0.000\n",
      "[48,  2000] loss: 0.000\n",
      "[48,  2200] loss: 0.000\n",
      "[48,  2400] loss: 0.000\n",
      "[48,  2600] loss: 0.000\n",
      "[48,  2800] loss: 0.000\n",
      "[48,  3000] loss: 0.000\n",
      "[48,  3200] loss: 0.000\n",
      "[48,  3400] loss: 0.000\n",
      "[48,  3600] loss: 0.000\n",
      "[48,  3800] loss: 0.000\n",
      "[48,  4000] loss: 0.000\n",
      "[48,  4200] loss: 0.000\n",
      "[48,  4400] loss: 0.000\n",
      "[48,  4600] loss: 0.000\n",
      "[48,  4800] loss: 0.000\n",
      "[48,  5000] loss: 0.000\n",
      "[48,  5200] loss: 0.000\n",
      "[48,  5400] loss: 0.000\n",
      "[48,  5600] loss: 0.000\n",
      "Accuracy : 98.603333 %\n",
      "Training epoch  48\n",
      "[49,   200] loss: 0.000\n",
      "[49,   400] loss: 0.000\n",
      "[49,   600] loss: 0.000\n",
      "[49,   800] loss: 0.000\n",
      "[49,  1000] loss: 0.000\n",
      "[49,  1200] loss: 0.000\n",
      "[49,  1400] loss: 0.000\n",
      "[49,  1600] loss: 0.000\n",
      "[49,  1800] loss: 0.000\n",
      "[49,  2000] loss: 0.000\n",
      "[49,  2200] loss: 0.000\n",
      "[49,  2400] loss: 0.000\n",
      "[49,  2600] loss: 0.000\n",
      "[49,  2800] loss: 0.000\n",
      "[49,  3000] loss: 0.000\n",
      "[49,  3200] loss: 0.000\n",
      "[49,  3400] loss: 0.000\n",
      "[49,  3600] loss: 0.000\n",
      "[49,  3800] loss: 0.000\n",
      "[49,  4000] loss: 0.000\n",
      "[49,  4200] loss: 0.000\n",
      "[49,  4400] loss: 0.000\n",
      "[49,  4600] loss: 0.000\n",
      "[49,  4800] loss: 0.000\n",
      "[49,  5000] loss: 0.000\n",
      "[49,  5200] loss: 0.000\n",
      "[49,  5400] loss: 0.000\n",
      "[49,  5600] loss: 0.000\n",
      "Accuracy : 98.603333 %\n",
      "Training epoch  49\n",
      "[50,   200] loss: 0.000\n",
      "[50,   400] loss: 0.000\n",
      "[50,   600] loss: 0.000\n",
      "[50,   800] loss: 0.000\n",
      "[50,  1000] loss: 0.000\n",
      "[50,  1200] loss: 0.000\n",
      "[50,  1400] loss: 0.000\n",
      "[50,  1600] loss: 0.000\n",
      "[50,  1800] loss: 0.000\n",
      "[50,  2000] loss: 0.000\n",
      "[50,  2200] loss: 0.000\n",
      "[50,  2400] loss: 0.000\n",
      "[50,  2600] loss: 0.000\n",
      "[50,  2800] loss: 0.000\n",
      "[50,  3000] loss: 0.000\n",
      "[50,  3200] loss: 0.000\n",
      "[50,  3400] loss: 0.000\n",
      "[50,  3600] loss: 0.000\n",
      "[50,  3800] loss: 0.000\n",
      "[50,  4000] loss: 0.000\n",
      "[50,  4200] loss: 0.000\n",
      "[50,  4400] loss: 0.000\n",
      "[50,  4600] loss: 0.000\n",
      "[50,  4800] loss: 0.000\n",
      "[50,  5000] loss: 0.000\n",
      "[50,  5200] loss: 0.000\n",
      "[50,  5400] loss: 0.000\n",
      "[50,  5600] loss: 0.000\n",
      "Accuracy : 98.673333 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    print('Training epoch ', epoch)\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    adjust_learning_rate(optimizer, epoch, 0.000390625, 10)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    test()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a202e4",
   "metadata": {},
   "source": [
    "batch= iter(trainloader)\n",
    "images, labels = batch.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110907de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3c07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82dc487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc425d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9552d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turbo 20 epoch: 62.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fe3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turbo 40 epoch: 65.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427fe189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1ec1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8d878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a612e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b30d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randint(10,(3,5,5))\n",
    "test_tp = torch.transpose(test,1,2)\n",
    "output = test+test_tp\n",
    "#print(output)\n",
    "test_2 = torch.randint(10,(3,6,6))\n",
    "test_2[:,2:4,2:4] = 0\n",
    "print(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize(64,64)\n",
    "0.7  50%\n",
    "0.84  53%\n",
    "\n",
    "#resize(128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73137d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GAN] *",
   "language": "python",
   "name": "conda-env-GAN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
