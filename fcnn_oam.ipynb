{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53d015bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "from scipy import signal\n",
    "from scipy import misc\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "373a4e4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (189609211.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [14]\u001b[1;36m\u001b[0m\n\u001b[1;33m    transforms.ToTensor()])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_path = \"f_d/train/\"\n",
    "test_path = \"f_d/test/\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [   #transforms.CenterCrop(128),\n",
    "        transforms.Resize([64,64])\n",
    "#        transforms.Resize([128,128]),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "train_transform = Compose([\n",
    "    #transforms.CenterCrop(128),\n",
    "        transforms.Resize([64,64])\n",
    "#    transforms.Resize([128,128]),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    #transforms.CenterCrop(128),\n",
    "        transforms.Resize([64,64])\n",
    "#    transforms.Resize([128,128]),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0, 0, 0], [1, 1, 1])\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_path,transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,drop_last=True,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=test_path,transform=transform)\n",
    "testloader =  DataLoader(testset, batch_size=BATCH_SIZE,drop_last=True,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "#classes = ('0', '1', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca13f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64])\n",
      "tensor(64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testbatch = iter(trainloader)\n",
    "testimages, testlabels = testbatch.next()\n",
    "\n",
    "print(testimages.shape)\n",
    "print(testlabels.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "x = 19\n",
    "img = testimages[x]\n",
    "img = torch.where(img > 0.89, img*0+1., img*0.)\n",
    "plt.imshow(img.numpy()[2])\n",
    "print(testlabels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3072c8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 22 11:01:32 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.79       Driver Version: 511.79       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 53%   44C    P3    80W / 370W |   2249MiB / 10240MiB |     29%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1192    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      1512    C+G   ...185.44\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      2608    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A      4436    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      5576    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5984    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8988    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9348    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      9748    C+G   ...batNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     12976    C+G                                   N/A      |\n",
      "|    0   N/A  N/A     13572    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     13828    C+G   ...Roaming\\Zoom\\bin\\Zoom.exe    N/A      |\n",
      "|    0   N/A  N/A     14796    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     14952    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     16248    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     16572    C+G   ....0.11.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     16964    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     17648    C+G   ...er_engine\\wallpaper32.exe    N/A      |\n",
      "|    0   N/A  N/A     18316    C+G   ...lack\\app-4.25.2\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A     19112    C+G   ...6bftszj\\TranslucentTB.exe    N/A      |\n",
      "|    0   N/A  N/A     21220    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     21412    C+G   ...iginThinSetupInternal.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78c09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          images, labels = data\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          outputs = net(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "#           _, predicted = torch.max(outputs.data, 1)  \n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('Accuracy : %f %%' % (\n",
    "      100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8432990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put net onto GPU\n",
      "FFTconv(\n",
      "  (conv1): FTconvlayer(3, 16, kernel_size=(128, 128), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc01): Linear(in_features=65536, out_features=256, bias=True)\n",
      "  (fc02): Linear(in_features=256, out_features=100, bias=True)\n",
      "  (drop_layer): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.nn.modules import Module\n",
    "from torch.nn.modules.utils import _single, _pair, _triple\n",
    "\n",
    "class _ConvNd(Module):\n",
    "\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups', 'bias',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size']   \n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, batch_size, stride,\n",
    "                 padding, dilation, transposed, output_padding,\n",
    "                 groups, bias, padding_mode):\n",
    "        super(_ConvNd, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                in_channels, out_channels // groups, *kernel_size))\n",
    "        else:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                out_channels, in_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        init.kaiming_normal_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.output_padding != (0,) * len(self.output_padding):\n",
    "            s += ', output_padding={output_padding}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(_ConvNd, self).__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "class FTconvlayer(_ConvNd):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, batch_size = 16, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros'):\n",
    "        batch_size = BATCH_SIZE\n",
    "        kernel_size = _pair(kernel_size) \n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(FTconvlayer, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, batch_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias, padding_mode)\n",
    "\n",
    "    def quantization_n(self, input, n = 1, max = 1):\n",
    "      intv = max/(2**n-1)\n",
    "      qunt = torch.ceil(torch.mul(input,(1/intv)))  #***use ceil instead of floor for small value of n to make sure that not all weights are modified to zero in the beginning, achieves good accuracy for small n\n",
    "      #the above line divide the whole tensor by the smallest interval (1/2**n-1), which is same as multiply with 2**n-1, then take the floor and finally multiply the whole tensor with the smallest interval\n",
    "      out = torch.mul(qunt,intv)\n",
    "      out = torch.clamp(out, min=0, max=max) #make sure the quantized version lies in the interval 0-1, if it's bigger than one just clamp it at one\n",
    "      return(out)  \n",
    "\n",
    "    def input_quant(self, input, level = 5):\n",
    "        max = torch.max(input)\n",
    "        intv = max/level\n",
    "        qunt = torch.floor(torch.mul(input,(1/intv)))\n",
    "        out = torch.mul(qunt, intv)\n",
    "        out = torch.clamp(out, min=0, max=max)\n",
    "        return(out)\n",
    "    \n",
    "    def weightclamp(self, input):\n",
    "      return input.clamp_(0)\n",
    "\n",
    "    def make_complex(self, x):  #converts a real tensor into complex form by adding one extra dimension to it\n",
    "        #x_i = torch.zeros(x.shape)\n",
    "        x_i = torch.cuda.FloatTensor(x.shape).fill_(0)\n",
    "        y = torch.stack((x,x_i),-1)\n",
    "        return torch.view_as_complex(y)\n",
    "\n",
    "    def neg_complex_exp(self, x):    #since pytorch does not support complex exponential, implemented using euler formula exp(-jx)=cos(-x)+jsin(-x)\n",
    "        x_cos = torch.cos(-x)\n",
    "        x_sin = torch.sin(-x)\n",
    "        x_euler = torch.stack((x_cos, x_sin), -1)\n",
    "        return torch.view_as_complex(x_euler)\n",
    "\n",
    "    def complex_mul(self, x,y):  #this implementation should support broadcasting \n",
    "        #result_r = x[...,0] * y[...,0]-x[...,1] * y[...,1]#real\n",
    "        #result_i = x[...,0] * y[...,1]+x[...,1] * y[...,0]#complex\n",
    "        #result = torch.stack((result_r,result_i),-1)#stack them together to get the result\n",
    "        result = x*y\n",
    "        return result\n",
    "\n",
    "    def conj_transpose(self, x):  #should support broadcasting\n",
    "        x = torch.view_as_real(x)\n",
    "        size = len(x.size())\n",
    "        x_r = x[...,0]\n",
    "        x_i = x[...,1]\n",
    "        x_i_c =-x_i\n",
    "        x_conj = torch.stack((x_r,x_i_c),-1)\n",
    "        x_conj_t = torch.transpose(x_conj, size-3, size-2)#size-1 is the dimension for complex representation\n",
    "        return torch.view_as_complex(x_conj_t)\n",
    "\n",
    "    def roll_n(self, X, axis, n):\n",
    "        f_idx = tuple(slice(None, None, None) if i != axis else slice(0, n, None) for i in range(X.dim()))\n",
    "        b_idx = tuple(slice(None, None, None) if i != axis else slice(n, None, None) for i in range(X.dim()))\n",
    "        front = X[f_idx]\n",
    "        back = X[b_idx]\n",
    "        return torch.cat([back, front], axis)   \n",
    "\n",
    "    def batch_fftshift2d(self, x):\n",
    "        real, imag = torch.unbind(x, -1)\n",
    "        for dim in range(len(real.size())-2, len(real.size())):\n",
    "            n_shift = real.size(dim)//2\n",
    "            if real.size(dim) % 2 != 0:\n",
    "                n_shift += 1  # for odd-sized images\n",
    "            real = self.roll_n(real, axis=dim, n=n_shift)\n",
    "            imag = self.roll_n(imag, axis=dim, n=n_shift)\n",
    "        return torch.stack((real, imag), -1)  # last dim=2 (real&imag)\n",
    "\n",
    "\n",
    "    def batch_ifftshift2d(self, x):\n",
    "        real, imag = torch.unbind(x, -1)\n",
    "        for dim in range(len(real.size()) - 1, len(real.size())-3, -1):\n",
    "            real = self.roll_n(real, axis=dim, n=real.size(dim)//2)\n",
    "            imag = self.roll_n(imag, axis=dim, n=imag.size(dim)//2)\n",
    "        return torch.stack((real, imag), -1)  # last dim=2 (real&imag)\n",
    "\n",
    "    def propTF(self, u1,L,lambdaa,z):\n",
    "        batch,M,N = u1.shape\n",
    "        dx = L/M\n",
    "        fx = torch.arange(-1/(2*dx),1/(2*dx), 1/L).cuda()\n",
    "        FX,FY = torch.meshgrid(fx,fx)\n",
    "        #H = torch.exp(-1j*math.pi*lambdaa*z*(FX**2+FY**2))\n",
    "        H = self.neg_complex_exp(math.pi*lambdaa*z*(FX**2+FY**2))\n",
    "        #H = self.batch_fftshift2d(H)\n",
    "        H = torch.fft.fftshift(H)\n",
    "        U1 = torch.fft.fft2(torch.fft.fftshift(u1)) #so the U1's dimension is 4, H is 3 so complex_mul needs to support broadcasting\n",
    "        U2 = self.complex_mul(H,U1)  ####GENERATES ERROR\n",
    "        u2 = torch.fft.ifftshift(torch.fft.ifft2(U2)) #this step is redundent, can be removed in actual implementation\n",
    "        return u2\n",
    "\n",
    "\n",
    "    def seidel_5(self, u0, v0, X, Y, wd, w040, w131, w222, w220, w311):\n",
    "        beta = math.atan2(u0,v0)\n",
    "        u0r=math.sqrt(u0**2+v0**2)\n",
    "        Xr=X*math.cos(beta)+Y*math.sin(beta)\n",
    "        Yr=-X*math.sin(beta)+Y*math.cos(beta)\n",
    "        rho2=Xr**2+Yr**2\n",
    "        w=wd*rho2+w040*rho2**2+w131*u0r*rho2*Xr+ w222*u0r**2*Xr**2+w220*u0r**2*rho2+w311*math.pow(u0r,3)*Xr\n",
    "        return w\n",
    "\n",
    "    def circ(self, r):\n",
    "        out = torch.abs(r)<=1\n",
    "        return out\n",
    "\n",
    "    ''' for block mean pytorch does not support reshape using 'F' ordering, so use normal reshape and then permute'''\n",
    "    def blockmean_batch(self, X, V, W):\n",
    "        S=X.shape\n",
    "        B1 = S[0]\n",
    "        B2 = S[1]\n",
    "        M = int(S[2] - S[2]%V)\n",
    "        N = int(S[3] - S[3]%W)\n",
    "        if(M*N == 0):\n",
    "            Y = X\n",
    "            return Y\n",
    "        MV = int(M/V)  \n",
    "        NW = int(N/W)\n",
    "        #XM = np.reshape(X[0:M, 0:N, :],(V, MV, W, NW, -1))\n",
    "        #XM =  X[0:M, 0:N].reshape(V, MV, W, NW, order=\"F\")\n",
    "        XM = X[:,:,0:M, 0:N].permute(0,1,3,2).reshape([B1,B2,NW, W, MV, V]).permute(0,1,5,4,3,2)\n",
    "        #three version of Y in matlab function depends on differen type of inputs, here stick to the double case\n",
    "        Y = torch.sum(torch.sum(XM,2),3) * (1/(V*W))\n",
    "        return Y\n",
    "\n",
    "    def extract_result(self,input,img_size):\n",
    "        size = input.shape[-1]\n",
    "        start = int((size-4*img_size)/2)\n",
    "        end = start + 4*img_size\n",
    "        output = input[:,:,start:end,start:end]\n",
    "        return output\n",
    "    \n",
    "    def input_pad(self,input,padsize):\n",
    "      input_size = input.shape[2]\n",
    "      pad_size_x = int((padsize-input_size)/2)\n",
    "      pad_size_y = int((padsize-input_size)/2)\n",
    "      p2d = (pad_size_x, pad_size_y, pad_size_x, pad_size_y)\n",
    "      input_pad = F.pad(input, p2d, \"constant\", 0)\n",
    "      return input_pad \n",
    "\n",
    "    def input_adjust(self, input):\n",
    "      input = torch.mul(input,5)\n",
    "      input = torch.floor(input)\n",
    "      output = torch.clamp(input,min=0,max=1)\n",
    "      return(output)\n",
    "\n",
    "    def evenkernel(self, input):\n",
    "      uptri = torch.triu(input,diagonal = 1)\n",
    "      downtri = torch.flip(torch.triu(input,diagonal = 1),[1,2])\n",
    "      result = uptri+downtri\n",
    "      return result\n",
    "\n",
    "    def extract_result(self,input,img_size):\n",
    "      size = input.shape[-1]\n",
    "      start = int((size-img_size)/2)\n",
    "      end = start + img_size\n",
    "      output = input[:,:,start:end,start:end]\n",
    "      return output\n",
    "    \n",
    "    def norm(self,input):\n",
    "      size = input.shape\n",
    "      output = torch.cuda.FloatTensor(size).fill_(0)\n",
    "      for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "          orig = input[i,j,:,:]\n",
    "          maxi = torch.max(orig)\n",
    "          mini = torch.min(orig)\n",
    "          output[i,j,:,:] = (orig-mini)/(maxi-mini)\n",
    "      return output\n",
    "\n",
    "    def kernel_even(self,input):\n",
    "      input_transpose = torch.transpose(input, 1,2)\n",
    "      input = input + input_transpose\n",
    "      return input\n",
    "\n",
    "    def kernel_hpf_even(self, input, amount):\n",
    "      # make the center part of the weight to be zero and make the filter symmetrical\n",
    "      # This function should be placed before quantization\n",
    "      # amount is the size of the area that are set to 0\n",
    "      mid = int(input.shape[2]/2)\n",
    "      input[:,mid-amount:mid+amount,mid-amount:mid+amount] = 0 #set the center part to be zero\n",
    "\n",
    "      input_transpose = torch.transpose(input, 1,2)\n",
    "      input = (input + input_transpose)/2\n",
    "      #even = input\n",
    "      return input\n",
    "\n",
    "    def accurate_model_forward(self, input, weight):\n",
    "        err = 1e-8 #define a very small error term to aviod nan loss due to abs\n",
    "        #print(input[0, ...])\n",
    "        #code to quantize the input to certain intervals to simulate dmd\n",
    "        #with torch.no_grad():\n",
    "          #input = self.input_quant(input)\n",
    "        res = input.shape[2]\n",
    "        xx = res\n",
    "        yy = xx \n",
    "        w = res    \n",
    "        #output_arr = torch.empty(32,16,w,w) #output dimension of the ftconv layer, hard-coded for easier implementation, 28 for MNIST and 32 for CIFAR\n",
    "        n_filter_actual = int(self.out_channels/2)\n",
    "        output_full = torch.cuda.FloatTensor(BATCH_SIZE,self.out_channels,w,w).fill_(0)\n",
    "        output_sub = torch.cuda.FloatTensor(BATCH_SIZE, int(self.out_channels/2), w, w).fill_(0)\n",
    "        # Define unit matrix DMD \n",
    "        '''unitmatrix = torch.cuda.FloatTensor(17, 17).fill_(0)\n",
    "        unitmatrix[1:16,1:16]=1\n",
    "        unitmatrix[7:10,7:10]=0\n",
    "        idledmd = unitmatrix.repeat(xx,yy)\n",
    "        '''\n",
    "        idledmd = torch.cuda.FloatTensor(res, res).fill_(1)\n",
    "        M,N = idledmd.shape\n",
    "        L1=1.90e-2*xx/res\n",
    "        L2=1.09e-2*yy/res\n",
    "        du=L1/M\n",
    "        dv=L2/N\n",
    "        lambdaa = 0.633e-6\n",
    "        k=2*math.pi/lambdaa\n",
    "        \n",
    "        '''Lens Diffraction (Aperture) and Aberration'''\n",
    "        fu = torch.arange(-1/(2*du),1/(2*du),1/L1)\n",
    "        #fv = torch.arange(-1/(2*dv),1/(2*dv),1/L2)\n",
    "        fv = torch.arange(-1/(2*dv),1/(2*dv),1/L2)\n",
    "        Dxp = 5e-2\n",
    "        wxp = Dxp/2\n",
    "        zxp = 200e-3\n",
    "        lz = lambdaa*zxp\n",
    "        u0 = 0\n",
    "        v0 = 0\n",
    "        f0 = wxp/(lambdaa*zxp)\n",
    "        '''Lens parameter for aberration (Seidel coefficients), wavefront alteration from spherical waves'''\n",
    "        wd=0*lambdaa\n",
    "        w040=4.963*lambdaa\n",
    "        w131=2.637*lambdaa\n",
    "        w222=9.025*lambdaa\n",
    "        w220=7.536*2*lambdaa\n",
    "        w311=0.157*12*lambdaa\n",
    "        \n",
    "        Fu,Fv = torch.meshgrid(fu,fv)\n",
    "        Fu = torch.transpose(Fu,0,1)\n",
    "        Fv = torch.transpose(Fv,0,1)\n",
    "        W = self.seidel_5(u0,v0,-lz*Fu/wxp,-lz*Fv/wxp,wd,w040,w131,w222,w220,w311).cuda() #same as the matlab calculation\n",
    "        #H = circ(torch.sqrt(Fu**2 + Fv**2)/f0)*torch.exp(-1j*k*W)#same as matlab calculation\n",
    "        H = self.complex_mul(self.make_complex(self.circ(torch.sqrt(Fu**2 + Fv**2)/f0).float().cuda()),self.neg_complex_exp(k*W))\n",
    "#-----------------------from here is the actual training, before the loop is basically constants/parameters genreation, which does not needs to be backproped     \n",
    "        for c_in in range(input.shape[1]): # iters for number of input channels\n",
    "            signal = input[:,c_in,:,:] #the dimension of signal is 3, with one batch dimension\n",
    "            weight_raw = weight[:,c_in,:,:] #the dimension of weights are now 3\n",
    "            #apply high pass filter and make kernel even\n",
    "            #weight_raw.data = self.kernel_even(weight_raw.data)\n",
    "            weight_raw.data = self.kernel_hpf_even(weight_raw.data,3)\n",
    "            weight_raw.data = self.quantization_n(weight_raw.data, 1, 1)\n",
    "            #print(weight_raw[0,...])\n",
    "            #weight_raw.data = torch.clamp(weight_raw.data, min=0, max=1)\n",
    "            '''interleave dimension needs to be changed since now the first dimension is batch dimension'''\n",
    "            #sw = torch.repeat_interleave(signal,4, dim=1)\n",
    "            #sw = torch.repeat_interleave(sw,4, dim=2)\n",
    "            dmd_1 = self.make_complex(self.input_pad(signal,res)) #dimension of dmd1 is now 4, first dimension is now batch dimension, so propTF needs to be changed accordingly\n",
    "            '''interleave dimension also needs to be changed here'''\n",
    "            #kk = torch.repeat_interleave(weight_raw,20, dim=1)#due to alignment reason, adjust the minimum unit of kernels to 50*50 pixles, so effective kernek size is 84*84\n",
    "            #kk = torch.repeat_interleave(kk,20, dim=2)\n",
    "            #dmd_kk = self.input_pad(kk,res)\n",
    "            #dmd_kk = self.evenkernel(weight_raw)\n",
    "            #dmd_kk = weight_raw\n",
    "            \n",
    "            #now need to implement propTF function\n",
    "            u2 = self.propTF(dmd_1,1.9e-2, lambdaa, 1.9e-2)\n",
    "            #u2 = dmd_1\n",
    "            \n",
    "            '''Fourier Transform after first lens'''\n",
    "            Gg = torch.fft.fftshift(torch.fft.fft2(u2))\n",
    "            Gi = self.complex_mul(Gg,self.conj_transpose(H))\n",
    "            Gi = Gi.unsqueeze(1)\n",
    "            \n",
    "            '''dot product in the fourier plane'''\n",
    "            Gii = self.complex_mul(Gi,self.make_complex(weight_raw))\n",
    "            '''Then get the result in real space'''\n",
    "            Grs = torch.fft.ifft2(torch.fft.ifftshift(Gii))\n",
    "            Grs = torch.view_as_real(Grs)\n",
    "            #Ii = torch.abs(Grs)**2\n",
    "            Ii = torch.sqrt(Grs[...,0]**2+Grs[...,1]**2+err)\n",
    "            #Ii = Grs[...,0]**2+Grs[...,1]**2 #take the squre to represent light intensity\n",
    "            op_abs = Ii#op_abs = self.extract_result(Ii,32)\n",
    "            output_full += op_abs\n",
    "        #output_sub = output_full[:, 0:n_filter_actual, :, :] - output_full[:, n_filter_actual:, :, :]\n",
    "        return output_full\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.accurate_model_forward(input, self.weight)\n",
    "        \n",
    "        #shape of input is [32,1,28,28], shape of kernel(weight) is [16,1,5,5]\n",
    "\n",
    "\n",
    "class FFTconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFTconv, self).__init__()\n",
    "        self.conv1 = FTconvlayer(3, 16, 128) #outdimension should be [32,16,28,28] (1/3,:,5/28/32) depends on whether training in fourier domain or no\n",
    "        #self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.fc01 = nn.Linear(16 * 64 * 64, 256)   #self.fc01 = nn.Linear(16 * 32 * 32, 256)\n",
    "        self.fc02 = nn.Linear(256, 100) #class number\n",
    "        self.drop_layer = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = torch.where(x > 0.9, x*0+1., x*0.)\n",
    "        x = self.bn1(self.pool1(self.conv1(x)))\n",
    "        x = x.view(64, 16 * 64 * 64)  #NEEDS UPDATE x = x.view(64, 16 * 32 * 32)\n",
    "        #x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc01(x))\n",
    "        #x = self.drop_layer(x)\n",
    "        x = self.fc02(x)\n",
    "        return x\n",
    "\n",
    "net = FFTconv()    \n",
    "\n",
    "\n",
    "if device:\n",
    "    net.to(device)\n",
    "    #torch.backends.cudnn.benchmark = True\n",
    "    print(\"put net onto GPU\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a263598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) #default learning rate for adam is 0.001\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, init_lr, freq):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every n epochs\"\"\"\n",
    "    lr = init_lr * (0.3 ** (epoch // freq))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f085b8e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\GAN\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.654\n",
      "[1,   400] loss: 0.046\n",
      "[1,   600] loss: 0.021\n",
      "[1,   800] loss: 0.013\n",
      "[1,  1000] loss: 0.010\n",
      "[1,  1200] loss: 0.015\n",
      "[1,  1400] loss: 0.024\n",
      "Accuracy : 48.543534 %\n",
      "Training epoch  1\n",
      "[2,   200] loss: 0.004\n",
      "[2,   400] loss: 0.001\n",
      "[2,   600] loss: 0.012\n",
      "[2,   800] loss: 0.003\n",
      "[2,  1000] loss: 0.012\n",
      "[2,  1200] loss: 0.020\n",
      "[2,  1400] loss: 0.030\n",
      "Accuracy : 51.188380 %\n",
      "Training epoch  2\n",
      "[3,   200] loss: 0.010\n",
      "[3,   400] loss: 0.004\n",
      "[3,   600] loss: 0.009\n",
      "[3,   800] loss: 0.008\n",
      "[3,  1000] loss: 0.017\n",
      "[3,  1200] loss: 0.022\n",
      "[3,  1400] loss: 0.030\n",
      "Accuracy : 51.702545 %\n",
      "Training epoch  3\n",
      "[4,   200] loss: 0.004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\GAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mFFTconv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m#x = self.pool1(F.relu(self.bn1(self.conv1(x))))\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.9\u001b[39m, x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1.\u001b[39m, x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.\u001b[39m)\n\u001b[1;32m--> 387\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    388\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m)  \u001b[38;5;66;03m#NEEDS UPDATE x = x.view(64, 16 * 32 * 32)\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;66;03m#x = self.drop_layer(x)\u001b[39;00m\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\GAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mFTconvlayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccurate_model_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mFTconvlayer.accurate_model_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m'''interleave dimension also needs to be changed here'''\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m#kk = torch.repeat_interleave(weight_raw,20, dim=1)#due to alignment reason, adjust the minimum unit of kernels to 50*50 pixles, so effective kernek size is 84*84\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m#kk = torch.repeat_interleave(kk,20, dim=2)\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m#dmd_kk = self.input_pad(kk,res)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m#now need to implement propTF function\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m u2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropTF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdmd_1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1.9e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdaa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.9e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m#u2 = dmd_1\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m'''Fourier Transform after first lens'''\u001b[39;00m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mFTconvlayer.propTF\u001b[1;34m(self, u1, L, lambdaa, z)\u001b[0m\n\u001b[0;32m    165\u001b[0m FX,FY \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(fx,fx)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m#H = torch.exp(-1j*math.pi*lambdaa*z*(FX**2+FY**2))\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg_complex_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlambdaa\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFX\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mFY\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m#H = self.batch_fftshift2d(H)\u001b[39;00m\n\u001b[0;32m    169\u001b[0m H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(H)\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mFTconvlayer.neg_complex_exp\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mneg_complex_exp\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):    \u001b[38;5;66;03m#since pytorch does not support complex exponential, implemented using euler formula exp(-jx)=cos(-x)+jsin(-x)\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     x_cos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m-\u001b[39mx)\n\u001b[1;32m--> 115\u001b[0m     x_sin \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m)\n\u001b[0;32m    116\u001b[0m     x_euler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((x_cos, x_sin), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mview_as_complex(x_euler)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    print('Training epoch ', epoch)\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    adjust_learning_rate(optimizer, epoch, 0.001, 10)\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "    test()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a202e4",
   "metadata": {},
   "source": [
    "batch= iter(trainloader)\n",
    "images, labels = batch.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b30d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randint(10,(3,5,5))\n",
    "test_tp = torch.transpose(test,1,2)\n",
    "output = test+test_tp\n",
    "#print(output)\n",
    "test_2 = torch.randint(10,(3,6,6))\n",
    "test_2[:,2:4,2:4] = 0\n",
    "print(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize(64,64)\n",
    "0.7  50%\n",
    "0.84  53%\n",
    "\n",
    "#resize(128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73137d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GAN] *",
   "language": "python",
   "name": "conda-env-GAN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
